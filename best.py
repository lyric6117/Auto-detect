"""
ğŸ”¥ å·¥ä¸šçº§å¼‚å¸¸æ£€æµ‹ç³»ç»Ÿ v2.0 - æ”¯æŒ ResNet + DINOv3
é›†æˆæ‰€æœ‰ä¼˜åŒ–ï¼šPatchCoreè½¯å¯¹é½ + Coreset + å¤šå°ºåº¦ + åœ¨çº¿å­¦ä¹ 
"""

import os
import sys
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, models
from PIL import Image
import faiss
from sklearn.neighbors import LocalOutlierFactor
from scipy.ndimage import gaussian_filter
import cv2
from tqdm import tqdm
import pickle
from typing import List, Tuple, Dict, Optional
import warnings
from datetime import timedelta

warnings.filterwarnings('ignore')


# ==================== é…ç½®å‚æ•° ====================
class Config:
    """DINOv3 ä¸“ç”¨é…ç½®"""
    # ========== è·¯å¾„é…ç½® ==========
    BASE_DIR = r"C:\Users\Administrator\Desktop\f-AnoGAN\RD4AD-main\mvtec\zhawa_guzhang_zhifangtujunhenghua"
    INITIAL_NORMAL_DIR = os.path.join(BASE_DIR, "train", "good")
    MIXED_DIR = r'D:\Huangda_data0801\mix'  #
    NORMAL_DIR = INITIAL_NORMAL_DIR
    TEST_DIR = MIXED_DIR
    OUTPUT_DIR = "./outputs"

    # ========== DINOv3 æ¨¡å‹é…ç½® ========== âœ… å…³é”®ä¿®å¤
    BACKBONE = "dinov3_vitl16"  # âœ… æ˜ç¡®ä½¿ç”¨DINOv3
    DINOV3_REPO_ROOT = r"D:\ly\dinov3-main"
    DINOV3_WEIGHT_PATH = r"D:\ly\dinov3-main\dinov3_vitl16_pretrain_lvd1689m-8aa4cbdd.pth"
    DINOV3_MODEL_NAME = "dinov3_vitl16_lvd1689m_distilled"  # å¯é€‰çš„å®Œæ•´åç§°
    DINOV3_FEATURE_LAYERS = [3, 6, 9, 11]  # ViT-L/16 æœ‰12å±‚ï¼Œé€‰æ‹©ä¸­é—´å±‚
    DINOV3_PATCH_SIZE = 16  # âœ… æ³¨æ„ï¼švitl16çš„patch sizeæ˜¯16

    # ========== é€šç”¨é…ç½® ==========
    IMAGE_SIZE = 224
    BATCH_SIZE = 512  # DINOv3æ˜¾å­˜å ç”¨å¤§
    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
    NUM_WORKERS = 12

    # ========== æ­¥éª¤1ï¼šç²—ç­›é…ç½® ==========
    AE_EPOCHS = 50
    AE_LR = 1e-3
    EXPAND_RATIO = 0.7

    # ========== æ­¥éª¤2ï¼šæ¸…æ´—é…ç½® ==========
    LOF_NEIGHBORS = 20
    CONTAMINATION = 0.05

    # ========== æ­¥éª¤3ï¼šFAISSé…ç½® ==========
    FAISS_NLIST = 100
    FAISS_NPROBE = 10
    USE_CORESET = True
    CORESET_RATIO = 0.1

    # ========== æ­¥éª¤6ï¼šå¼‚å¸¸è¯„åˆ†é…ç½® ==========
    N_NEIGHBORS = 9
    ANOMALY_PERCENTILE = 90

# ==================== æ•°æ®é›† ====================
class ImageDataset(Dataset):
    def __init__(self, image_dir: str, transform=None, image_size=224):  # âœ… æ·»åŠ å‚æ•°
        self.image_paths = []
        for root, _, files in os.walk(image_dir):
            for f in files:
                if f.lower().endswith(('.jpg', '.png', '.jpeg', '.bmp')):
                    self.image_paths.append(os.path.join(root, f))

        if len(self.image_paths) == 0:
            raise ValueError(f"æœªåœ¨ {image_dir} ä¸­æ‰¾åˆ°å›¾ç‰‡ï¼")

        self.transform = transform
        self.image_size = image_size  # âœ… ä¿å­˜
        print(f"   åŠ è½½ {len(self.image_paths)} å¼ å›¾ç‰‡")

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        try:
            img = Image.open(self.image_paths[idx]).convert('RGB')
            if self.transform:
                img = self.transform(img)
            return img, self.image_paths[idx]
        except Exception as e:
            print(f"âš ï¸  åŠ è½½å›¾ç‰‡å¤±è´¥: {self.image_paths[idx]} - {e}")
            # âœ… ä½¿ç”¨åŠ¨æ€å°ºå¯¸
            return torch.zeros(3, self.image_size, self.image_size), self.image_paths[idx]

class PathDataset(Dataset):
    def __init__(self, paths, transform=None):
        self.paths = paths
        self.transform = transform

    def __len__(self):
        return len(self.paths)

    def __getitem__(self, idx):
        img_path = self.paths[idx]
        img = Image.open(img_path).convert("RGB")
        if self.transform:
            img = self.transform(img)
        return img
# ==================== æ­¥éª¤1ï¼šè‡ªç›‘ç£ç²—ç­› ====================
class SimpleAutoEncoder(nn.Module):
    """è½»é‡çº§è‡ªç¼–ç å™¨"""

    def __init__(self):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Conv2d(3, 64, 4, stride=2, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 128, 4, stride=2, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 256, 4, stride=2, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 512, 4, stride=2, padding=1),
        )
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(512, 256, 4, stride=2, padding=1),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(64, 3, 4, stride=2, padding=1),
            nn.Sigmoid()
        )

    def forward(self, x):
        return self.decoder(self.encoder(x))


def train_autoencoder(dataloader, config):
    """è®­ç»ƒè‡ªç¼–ç å™¨"""
    print("\n" + "=" * 60)
    print("ğŸ“Œ [æ­¥éª¤1] è®­ç»ƒè‡ªç¼–ç å™¨è¿›è¡Œç²—ç­›")
    print("=" * 60)

    model = SimpleAutoEncoder().to(config.DEVICE)
    optimizer = torch.optim.Adam(model.parameters(), lr=config.AE_LR)
    criterion = nn.MSELoss()

    model.train()
    for epoch in range(config.AE_EPOCHS):
        losses = []
        pbar = tqdm(dataloader, desc=f"Epoch {epoch + 1}/{config.AE_EPOCHS}")
        for imgs, _ in pbar:
            imgs = imgs.to(config.DEVICE)
            recon = model(imgs)
            loss = criterion(recon, imgs)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            losses.append(loss.item())
            pbar.set_postfix({'loss': f"{np.mean(losses):.4f}"})

        print(f"   Epoch {epoch + 1}/{config.AE_EPOCHS} - Loss: {np.mean(losses):.4f}")

    return model


def expand_normal_gallery(ae_model, mixed_dataloader, config):
    """æ‰©å……Normal Gallery"""
    print("\n   ä½¿ç”¨AEç­›é€‰æ­£å¸¸å›¾...")
    ae_model.eval()
    reconstruction_errors = []
    image_paths = []

    with torch.no_grad():
        for imgs, paths in tqdm(mixed_dataloader, desc="   è®¡ç®—é‡å»ºè¯¯å·®"):
            imgs = imgs.to(config.DEVICE)
            recon = ae_model(imgs)
            errors = F.mse_loss(recon, imgs, reduction='none')
            errors = errors.view(imgs.size(0), -1).mean(dim=1)
            reconstruction_errors.extend(errors.cpu().numpy())
            image_paths.extend(paths)

    errors = np.array(reconstruction_errors)
    threshold = np.percentile(errors, config.EXPAND_RATIO * 100)
    selected_indices = np.where(errors <= threshold)[0]

    expanded_paths = [image_paths[i] for i in selected_indices]
    print(f"   âœ… ä» {len(image_paths)} å¼ æ··åˆå›¾ä¸­ç­›é€‰å‡º {len(expanded_paths)} å¼ æ­£å¸¸å›¾")
    print(f"   âœ… é‡å»ºè¯¯å·®é˜ˆå€¼: {threshold:.6f}")

    return expanded_paths, errors


# ==================== æ­¥éª¤2ï¼šå¤šå±‚ç‰¹å¾æå–å™¨ ====================
class FeatureExtractor(nn.Module):
    """ğŸ”¥ DINOv3 ä¸“ç”¨ç‰¹å¾æå–å™¨"""

    def __init__(self, config: Config):
        super().__init__()
        self.config = config
        self.backbone_type = self._parse_backbone_type()

        if self.backbone_type == 'dinov3':
            self.model, self.feature_layers = self._load_dinov3()
        else:
            raise ValueError(f"å½“å‰ä»…æ”¯æŒDINOv3ï¼Œæ”¶åˆ°: {config.BACKBONE}")

        self.model.eval()
        for p in self.model.parameters():
            p.requires_grad = False

    def _parse_backbone_type(self) -> str:
        """åˆ¤æ–­backboneç±»å‹"""
        backbone = self.config.BACKBONE.lower()
        if 'dinov3' in backbone or 'dino_v3' in backbone:
            return 'dinov3'
        else:
            raise ValueError(f"è¯·ä½¿ç”¨DINOv3æ¨¡å‹ï¼Œå½“å‰é…ç½®: {self.config.BACKBONE}")

    def _load_dinov3(self):
        """ğŸ”¥ ä»æœ¬åœ°åŠ è½½DINOv3æ¨¡å‹"""
        print("\n" + "=" * 60)
        print("ğŸ“¥ ä»æœ¬åœ°åŠ è½½ DINOv3 æ¨¡å‹")
        print("=" * 60)

        repo_root = self.config.DINOV3_REPO_ROOT
        weight_path = self.config.DINOV3_WEIGHT_PATH

        # éªŒè¯è·¯å¾„
        if not os.path.exists(repo_root):
            raise FileNotFoundError(f"DINOv3ä»“åº“è·¯å¾„ä¸å­˜åœ¨: {repo_root}")
        if not os.path.exists(weight_path):
            raise FileNotFoundError(f"æƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {weight_path}")

        print(f"   ğŸ“‚ ä»“åº“è·¯å¾„: {repo_root}")
        print(f"   ğŸ“‚ æƒé‡è·¯å¾„: {weight_path}")

        # æ·»åŠ è·¯å¾„
        if repo_root not in sys.path:
            sys.path.insert(0, repo_root)

        try:
            # æ–¹æ³•1ï¼šç›´æ¥å¯¼å…¥ï¼ˆé€‚ç”¨äºæ–°ç‰ˆDINOv3ï¼‰
            print("\n   ğŸ”„ å°è¯•æ–¹æ³•1: ç›´æ¥å¯¼å…¥æ¨¡å‹...")
            from dinov3.models.vision_transformer import vit_large

            # åˆ›å»ºæ¨¡å‹
            model = vit_large(
                patch_size=self.config.DINOV3_PATCH_SIZE,
                num_register_tokens=0,
                interpolate_antialias=False,
                interpolate_offset=0.1,
            )

            # åŠ è½½æƒé‡
            print(f"   ğŸ“¥ åŠ è½½æƒé‡...")
            state_dict = torch.load(weight_path, map_location='cpu')

            # å¤„ç†æƒé‡é”®åï¼ˆå¯èƒ½éœ€è¦å»æ‰'teacher'å‰ç¼€ï¼‰
            if 'teacher' in state_dict:
                state_dict = state_dict['teacher']
            elif 'model' in state_dict:
                state_dict = state_dict['model']

            # å»æ‰'module.'å‰ç¼€ï¼ˆå¦‚æœæœ‰ï¼‰
            new_state_dict = {}
            for k, v in state_dict.items():
                new_key = k.replace('module.', '').replace('backbone.', '')
                new_state_dict[new_key] = v

            msg = model.load_state_dict(new_state_dict, strict=False)
            print(f"   âœ… æƒé‡åŠ è½½å®Œæˆ")
            if msg.missing_keys:
                print(f"   âš ï¸  ç¼ºå¤±é”®: {msg.missing_keys[:5]}...")
            if msg.unexpected_keys:
                print(f"   âš ï¸  å¤šä½™é”®: {msg.unexpected_keys[:5]}...")

        except Exception as e1:
            print(f"   âŒ æ–¹æ³•1å¤±è´¥: {e1}")
            print(f"\n   ğŸ”„ å°è¯•æ–¹æ³•2: ä½¿ç”¨torch.loadç›´æ¥åŠ è½½...")

            try:
                # æ–¹æ³•2ï¼šå¦‚æœæƒé‡æ–‡ä»¶åŒ…å«å®Œæ•´æ¨¡å‹
                checkpoint = torch.load(weight_path, map_location='cpu')

                if isinstance(checkpoint, dict):
                    if 'model' in checkpoint:
                        model = checkpoint['model']
                    elif 'teacher' in checkpoint:
                        model = checkpoint['teacher']
                    else:
                        raise ValueError("æ— æ³•ä»checkpointä¸­æ‰¾åˆ°æ¨¡å‹")
                else:
                    model = checkpoint

                print(f"   âœ… ç›´æ¥åŠ è½½æ¨¡å‹æˆåŠŸ")

            except Exception as e2:
                print(f"   âŒ æ–¹æ³•2ä¹Ÿå¤±è´¥: {e2}")

                # æ–¹æ³•3ï¼šæœ€åçš„å°è¯• - ä½¿ç”¨dinov3ä»“åº“çš„builder
                print(f"\n   ğŸ”„ å°è¯•æ–¹æ³•3: ä½¿ç”¨å®˜æ–¹builder...")
                try:
                    # è¿™ä¸ªéœ€è¦æ ¹æ®ä½ çš„dinov3ä»“åº“ç»“æ„è°ƒæ•´
                    import dinov3
                    from dinov3.models import build_model

                    # æ„å»ºé…ç½®
                    model_config = {
                        'arch': 'vit_large',
                        'patch_size': self.config.DINOV3_PATCH_SIZE,
                        'pretrained_weights': weight_path,
                    }

                    model = build_model(model_config)
                    print(f"   âœ… ä½¿ç”¨builderåŠ è½½æˆåŠŸ")

                except Exception as e3:
                    raise RuntimeError(
                        f"æ‰€æœ‰åŠ è½½æ–¹æ³•éƒ½å¤±è´¥äº†ï¼\n"
                        f"æ–¹æ³•1: {e1}\n"
                        f"æ–¹æ³•2: {e2}\n"
                        f"æ–¹æ³•3: {e3}\n"
                        f"è¯·æ£€æŸ¥:\n"
                        f"1. DINOv3ä»“åº“ç»“æ„æ˜¯å¦å®Œæ•´\n"
                        f"2. æƒé‡æ–‡ä»¶æ˜¯å¦åŒ¹é…æ¨¡å‹æ¶æ„\n"
                        f"3. ä¾èµ–åŒ…æ˜¯å¦å®‰è£…å®Œæ•´"
                    )

        # ç§»åŠ¨åˆ°è®¾å¤‡
        model = model.to(self.config.DEVICE)

        # æ³¨å†Œç‰¹å¾æå–hook
        feature_layers = self.config.DINOV3_FEATURE_LAYERS
        self.features = {}

        print(f"\n   ğŸ”§ æ³¨å†Œç‰¹å¾æå–hook...")
        print(f"   ğŸ“Š æ¨¡å‹æ€»å±‚æ•°: {len(model.blocks)}")
        print(f"   ğŸ“Œ æå–å±‚ç´¢å¼•: {feature_layers}")

        for layer_idx in feature_layers:
            if layer_idx >= len(model.blocks):
                raise ValueError(f"å±‚ç´¢å¼•{layer_idx}è¶…å‡ºèŒƒå›´(æ€»å…±{len(model.blocks)}å±‚)")

            def make_hook(idx):
                def hook(module, input, output):
                    self.features[f'block_{idx}'] = output

                return hook

            model.blocks[layer_idx].register_forward_hook(make_hook(layer_idx))

        print(f"   âœ… Hookæ³¨å†Œå®Œæˆ")
        print("=" * 60)

        return model, [f'block_{i}' for i in feature_layers]

    def forward(self, x):
        """å‰å‘ä¼ æ’­ - DINOv3ä¸“ç”¨"""
        # DINOv3çš„è¾“å‡ºæ˜¯ [B, N+1, D]
        # N = (H/patch_size) * (W/patch_size)
        # +1 æ˜¯CLS token

        _ = self.model(x)

        # æå–å¹¶å¤„ç†ç‰¹å¾
        output_features = {}
        B = x.shape[0]
        patch_size = self.config.DINOV3_PATCH_SIZE
        H = W = self.config.IMAGE_SIZE // patch_size  # 224/16 = 14

        for layer_name, feat in self.features.items():
            # feat shape: [B, N+1, D]
            # å»æ‰CLS token: [B, N, D]
            feat = feat[:, 1:, :]
            D = feat.shape[-1]

            # reshapeä¸º [B, D, H, W]
            feat = feat.transpose(1, 2).reshape(B, D, H, W)
            output_features[layer_name] = feat

        return output_features

def get_optimized_dataloader(dataset, config, shuffle=False):
    """ç»Ÿä¸€çš„DataLoaderé…ç½®"""
    return DataLoader(
        dataset,
        batch_size=config.BATCH_SIZE,
        shuffle=shuffle,
        num_workers=config.NUM_WORKERS,  # 12ä¸ªworker
        pin_memory=True,                 # âœ… GPUåŠ é€Ÿ
        persistent_workers=True,         # âœ… workeræŒä¹…åŒ–
        prefetch_factor=2,               # âœ… æ¯ä¸ªworkeré¢„åŠ è½½2ä¸ªbatch
        drop_last=False
    )
def extract_features(dataloader, feature_extractor, config):
    """æå–å¤šå±‚patchç‰¹å¾"""
    print("\n" + "=" * 60)
    print("ğŸ“Œ [æ­¥éª¤2] æå–å¤šå±‚ç‰¹å¾")
    print("=" * 60)

    all_features = []
    all_paths = []

    with torch.no_grad():
        for imgs, paths in tqdm(dataloader, desc="   æå–ç‰¹å¾"):
            imgs = imgs.to(config.DEVICE)
            features = feature_extractor(imgs)

            # èåˆå¤šå±‚ç‰¹å¾
            embeddings = []
            for layer in feature_extractor.feature_layers:
                feat = features[layer]
                # ç»Ÿä¸€è°ƒæ•´åˆ°16x16å¤§å°
                feat = F.adaptive_avg_pool2d(feat, (16, 16))
                embeddings.append(feat)

            # æ‹¼æ¥
            embedding = torch.cat(embeddings, dim=1)  # [B, C_total, 16, 16]
            B, C, H, W = embedding.shape

            # è½¬æ¢ä¸º [B, H*W, C]
            embedding = embedding.permute(0, 2, 3, 1).reshape(B, H * W, C)

            all_features.append(embedding.cpu().numpy())
            all_paths.extend(paths)

    all_features = np.concatenate(all_features, axis=0)  # [N, 256, C]
    print(f"   âœ… æå–ç‰¹å¾å½¢çŠ¶: {all_features.shape}")
    print(f"   âœ… ç‰¹å¾ç»´åº¦: {all_features.shape[-1]} (å¤šå±‚èåˆ)")

    return all_features, all_paths


def clean_gallery_with_lof(features, paths, config):
    """ä½¿ç”¨LOFæ¸…æ´—"""
    print("\n" + "=" * 60)
    print("ğŸ“Œ [æ­¥éª¤2] ä½¿ç”¨LOFæ¸…æ´—Gallery")
    print("=" * 60)

    global_features = features.mean(axis=1)

    lof = LocalOutlierFactor(
        n_neighbors=config.LOF_NEIGHBORS,
        contamination=config.CONTAMINATION,
        n_jobs=-1
    )

    print(f"   ğŸ” LOFå‚æ•°: n_neighbors={config.LOF_NEIGHBORS}, contamination={config.CONTAMINATION}")
    labels = lof.fit_predict(global_features)

    clean_indices = np.where(labels == 1)[0]
    outlier_indices = np.where(labels == -1)[0]

    clean_features = features[clean_indices]
    clean_paths = [paths[i] for i in clean_indices]

    print(f"   âœ… æ¸…æ´—å‰: {len(paths)} å¼ ")
    print(f"   âœ… æ¸…æ´—å: {len(clean_paths)} å¼  (ä¿ç•™ {len(clean_paths) / len(paths) * 100:.1f}%)")
    print(f"   ğŸ—‘ï¸  å‰”é™¤ç¦»ç¾¤ç‚¹: {len(outlier_indices)} å¼ ")

    return clean_features, clean_paths


# ==================== æ­¥éª¤3ï¼šCoreset + FAISS ====================
def greedy_coreset_sampling(features: np.ndarray, ratio: float = 0.1):
    """ğŸ”¥ ä¼˜åŒ–1ï¼šè´ªå¿ƒCoreseté‡‡æ ·"""
    print("\n" + "=" * 60)
    print(f"ğŸ“Œ [æ­¥éª¤3] Coreseté‡‡æ · (ä¿ç•™ {ratio * 100:.1f}%)")
    print("=" * 60)

    N, D = features.shape
    target_n = max(int(N * ratio), 1000)  # è‡³å°‘ä¿ç•™1000ä¸ª

    if target_n >= N:
        print(f"   âš ï¸  ç›®æ ‡æ•°é‡({target_n}) >= æ€»æ•°({N})ï¼Œè·³è¿‡é‡‡æ ·")
        return np.arange(N)

    selected_indices = [np.random.randint(N)]
    min_distances = np.full(N, np.inf)

    for _ in tqdm(range(1, target_n), desc="   è´ªå¿ƒé‡‡æ ·"):
        last_selected = features[selected_indices[-1]]
        distances = np.linalg.norm(features - last_selected, axis=1)
        min_distances = np.minimum(min_distances, distances)
        next_idx = np.argmax(min_distances)
        selected_indices.append(next_idx)

    print(f"   âœ… é‡‡æ ·å®Œæˆ: {len(selected_indices)} / {N} patches")
    return np.array(selected_indices)


def build_faiss_index(features, config):
    """æ„å»ºFAISSç´¢å¼•"""
    print("\n   ğŸ”¨ æ„å»ºFAISSç´¢å¼•...")

    N_images, N_patches, D = features.shape
    patch_features = features.reshape(-1, D).astype('float32')

    print(f"   ğŸ“Š åŸå§‹patchæ•°: {patch_features.shape[0]:,}")

    # Coreseté‡‡æ ·
    if config.USE_CORESET and patch_features.shape[0] > 10000:
        coreset_indices = greedy_coreset_sampling(patch_features, config.CORESET_RATIO)
        patch_features = patch_features[coreset_indices]

    # å½’ä¸€åŒ–
    faiss.normalize_L2(patch_features)

    # æ„å»ºç´¢å¼•
    if patch_features.shape[0] < 50000:
        index = faiss.IndexFlatL2(D)
        index.add(patch_features)
        print(f"   âœ… ä½¿ç”¨ç²¾ç¡®ç´¢å¼• (IndexFlatL2)")
    else:
        quantizer = faiss.IndexFlatL2(D)
        index = faiss.IndexIVFPQ(quantizer, D, config.FAISS_NLIST, 64, 8)
        index.train(patch_features)
        index.add(patch_features)
        index.nprobe = config.FAISS_NPROBE
        print(f"   âœ… ä½¿ç”¨IVF-PQç´¢å¼• (nlist={config.FAISS_NLIST})")

    print(f"   âœ… ç´¢å¼•åŒ…å« {index.ntotal:,} ä¸ªpatchç‰¹å¾")

    return index, patch_features


# ==================== æ­¥éª¤6ï¼šå¼‚å¸¸è¯„åˆ† ====================
def compute_anomaly_scores(test_features, faiss_index, config):
    """ğŸ”¥ ä¼˜åŒ–2ï¼šPatchCoreè½¯å¯¹é½"""
    print("\n" + "=" * 60)
    print("ğŸ“Œ [æ­¥éª¤4-6] è®¡ç®—å¼‚å¸¸åˆ†æ•° (PatchCoreæ–¹å¼)")
    print("=" * 60)

    N_test, N_patches, D = test_features.shape
    anomaly_scores = []
    anomaly_maps = []

    for i in tqdm(range(N_test), desc="   è®¡ç®—åˆ†æ•°"):
        query = test_features[i].astype('float32')
        faiss.normalize_L2(query)

        # k-NNæœç´¢
        distances, _ = faiss_index.search(query, config.N_NEIGHBORS)

        # æ¯ä¸ªpatchçš„å¼‚å¸¸åˆ†æ•°
        patch_scores = distances.mean(axis=1)

        # å›¾åƒçº§åˆ†æ•°
        image_score = patch_scores.max()
        anomaly_scores.append(image_score)

        # çƒ­åŠ›å›¾
        h = w = int(np.sqrt(N_patches))
        anomaly_map = patch_scores.reshape(h, w)
        anomaly_maps.append(anomaly_map)

    scores = np.array(anomaly_scores)
    print(f"   âœ… å¼‚å¸¸åˆ†æ•°èŒƒå›´: [{scores.min():.4f}, {scores.max():.4f}]")
    print(f"   âœ… å¼‚å¸¸åˆ†æ•°å‡å€¼: {scores.mean():.4f}")

    return scores, anomaly_maps


def generate_anomaly_heatmap(anomaly_map, original_img_path, save_path):
    """ç”Ÿæˆå¼‚å¸¸çƒ­åŠ›å›¾"""
    img = cv2.imread(original_img_path)
    if img is None:
        print(f"âš ï¸  æ— æ³•è¯»å–å›¾ç‰‡: {original_img_path}")
        return

    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    H, W = img.shape[:2]

    # ä¸Šé‡‡æ ·
    heatmap = cv2.resize(anomaly_map, (W, H))
    heatmap = gaussian_filter(heatmap, sigma=4)

    # å½’ä¸€åŒ–
    heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min() + 1e-8)
    heatmap = (heatmap * 255).astype(np.uint8)

    # é¢œè‰²æ˜ å°„
    heatmap_color = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)

    # å åŠ 
    superimposed = cv2.addWeighted(img, 0.6, heatmap_color, 0.4, 0)

    cv2.imwrite(save_path, cv2.cvtColor(superimposed, cv2.COLOR_RGB2BGR))


# ==================== ä¸»æµç¨‹ ====================
class AnomalyDetectionPipeline:
    def __init__(self, config: Config):
        self.config = config
        os.makedirs(config.OUTPUT_DIR, exist_ok=True)

        self.transform = transforms.Compose([
            transforms.Resize((config.IMAGE_SIZE, config.IMAGE_SIZE)),
            transforms.ToTensor(),
            transforms.Normalize(
                mean=[0.485, 0.456, 0.406],
                std=[0.229, 0.224, 0.225]
            )
        ])

    def run(self):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        print("\n" + "=" * 60)
        print("ğŸš€ å¼€å§‹è®­ç»ƒæµç¨‹")
        print("=" * 60)

        # æ­¥éª¤1
        print(f"\nğŸ“‚ åŠ è½½åˆå§‹æ­£å¸¸å›¾: {self.config.INITIAL_NORMAL_DIR}")
        initial_dataset = ImageDataset(self.config.INITIAL_NORMAL_DIR, self.transform)
        initial_loader = get_optimized_dataloader(
            initial_dataset, self.config, shuffle=True
        )  # âœ… ä½¿ç”¨ä¼˜åŒ–çš„loader

        ae_model = train_autoencoder(initial_loader, self.config)

        print(f"\nğŸ“‚ åŠ è½½æ··åˆå›¾: {self.config.MIXED_DIR}")
        mixed_dataset = ImageDataset(self.config.MIXED_DIR, self.transform)
        mixed_loader = get_optimized_dataloader(
            mixed_dataset, self.config, shuffle=False
        )  # âœ… ä½¿ç”¨ä¼˜åŒ–çš„loader

        expanded_paths, _ = expand_normal_gallery(ae_model, mixed_loader, self.config)

        # æ­¥éª¤2
        all_normal_paths = initial_dataset.image_paths + expanded_paths


        gallery_dataset = PathDataset(all_normal_paths, self.transform)
        gallery_loader = get_optimized_dataloader(
            gallery_dataset, self.config, shuffle=False
        )  # âœ… ä½¿ç”¨ä¼˜åŒ–çš„loader

        feature_extractor = FeatureExtractor(self.config).to(self.config.DEVICE)

        gallery_features, gallery_paths = extract_features(
            gallery_loader, feature_extractor, self.config
        )

        clean_features, clean_paths = clean_gallery_with_lof(
            gallery_features, gallery_paths, self.config
        )

        # æ­¥éª¤3
        faiss_index, memory_bank = build_faiss_index(clean_features, self.config)

        # ä¿å­˜
        model_save_path = os.path.join(self.config.OUTPUT_DIR, 'model.pkl')
        with open(model_save_path, 'wb') as f:
            pickle.dump({
                'faiss_index': faiss.serialize_index(faiss_index),
                'clean_paths': clean_paths,
                'config': self.config
            }, f)

        print("\n" + "=" * 60)
        print(f"âœ… è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜è‡³: {model_save_path}")
        print("=" * 60)

        return feature_extractor, faiss_index, clean_paths

    def inference(self, test_dir: str, feature_extractor, faiss_index):
        """æ¨ç†"""
        print("\n" + "=" * 60)
        print("ğŸ” å¼€å§‹å¼‚å¸¸æ£€æµ‹")
        print("=" * 60)

        test_dataset = ImageDataset(test_dir, self.transform)
        test_loader = DataLoader(
            test_dataset,
            batch_size=self.config.BATCH_SIZE,
            shuffle=False,
            num_workers=self.config.NUM_WORKERS
        )

        test_features, test_paths = extract_features(
            test_loader, feature_extractor, self.config
        )

        scores, anomaly_maps = compute_anomaly_scores(
            test_features, faiss_index, self.config
        )

        # åŠ¨æ€é˜ˆå€¼
        threshold = np.percentile(scores, self.config.ANOMALY_PERCENTILE)
        print(f"\n   ğŸ¯ åŠ¨æ€é˜ˆå€¼ ({self.config.ANOMALY_PERCENTILE}%åˆ†ä½): {threshold:.4f}")

        # æ’åº
        sorted_indices = np.argsort(scores)[::-1]

        results = []
        for rank, idx in enumerate(sorted_indices):
            results.append({
                'rank': rank + 1,
                'path': test_paths[idx],
                'score': scores[idx],
                'is_anomaly': scores[idx] > threshold
            })

        # ä¿å­˜ç»“æœ
        result_path = os.path.join(self.config.OUTPUT_DIR, 'anomaly_results.txt')
        with open(result_path, 'w', encoding='utf-8') as f:
            f.write(f"{'Rank':<6} {'Score':<12} {'Status':<12} {'Path'}\n")
            f.write("=" * 80 + "\n")
            for r in results[:100]:
                status = 'ğŸ”´ ANOMALY' if r['is_anomaly'] else 'ğŸŸ¢ NORMAL'
                f.write(f"{r['rank']:<6} {r['score']:<12.6f} {status:<12} {r['path']}\n")

        print(f"\n   âœ… ç»“æœå·²ä¿å­˜: {result_path}")

        # çƒ­åŠ›å›¾
        heatmap_dir = os.path.join(self.config.OUTPUT_DIR, 'heatmaps')
        os.makedirs(heatmap_dir, exist_ok=True)

        print("\n   ğŸ¨ ç”Ÿæˆçƒ­åŠ›å›¾...")
        for i, idx in enumerate(tqdm(sorted_indices[:20], desc="   ç”Ÿæˆä¸­")):
            save_path = os.path.join(
                heatmap_dir,
                f"rank{i + 1}_score{scores[idx]:.4f}.jpg"
            )
            generate_anomaly_heatmap(
                anomaly_maps[idx],
                test_paths[idx],
                save_path
            )

        print(f"   âœ… çƒ­åŠ›å›¾å·²ä¿å­˜: {heatmap_dir}")

        # ç»Ÿè®¡
        n_anomalies = sum(r['is_anomaly'] for r in results)
        print("\n" + "=" * 60)
        print(f"ğŸ“Š æ£€æµ‹ç»Ÿè®¡")
        print("=" * 60)
        print(f"   æ€»è®¡: {len(results)} å¼ ")
        print(f"   å¼‚å¸¸: {n_anomalies} å¼  ({n_anomalies / len(results) * 100:.1f}%)")
        print(f"   æ­£å¸¸: {len(results) - n_anomalies} å¼ ")

        return results


# ==================== ğŸ”¥ ä¼˜åŒ–4ï¼šåœ¨çº¿å­¦ä¹  ====================
def incremental_update(new_normal_dir: str, model_path: str):
    """å¢é‡æ›´æ–°Normal Gallery"""
    print("\n" + "=" * 60)
    print("ğŸ”„ å¢é‡æ›´æ–°Normal Gallery")
    print("=" * 60)

    # åŠ è½½æ¨¡å‹
    with open(model_path, 'rb') as f:
        data = pickle.load(f)

    faiss_index = faiss.deserialize_index(data['faiss_index'])
    clean_paths = data['clean_paths']
    config = data['config']

    print(f"   ğŸ“‚ å½“å‰Gallery: {len(clean_paths)} å¼ ")

    # æå–æ–°æ ·æœ¬ç‰¹å¾
    feature_extractor = FeatureExtractor(config).to(config.DEVICE)

    transform = transforms.Compose([
        transforms.Resize((config.IMAGE_SIZE, config.IMAGE_SIZE)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    new_dataset = ImageDataset(new_normal_dir, transform)
    new_loader = DataLoader(
        new_dataset,
        batch_size=config.BATCH_SIZE,
        shuffle=False,
        num_workers=config.NUM_WORKERS
    )

    new_features, new_paths = extract_features(new_loader, feature_extractor, config)

    # æ·»åŠ åˆ°ç´¢å¼•
    N, P, D = new_features.shape
    new_patch_features = new_features.reshape(-1, D).astype('float32')
    faiss.normalize_L2(new_patch_features)
    faiss_index.add(new_patch_features)

    # æ›´æ–°è·¯å¾„
    clean_paths.extend(new_paths)

    # ä¿å­˜
    with open(model_path, 'wb') as f:
        pickle.dump({
            'faiss_index': faiss.serialize_index(faiss_index),
            'clean_paths': clean_paths,
            'config': config
        }, f)

    print(f"   âœ… æ–°å¢ {len(new_paths)} å¼ æ­£å¸¸å›¾")
    print(f"   âœ… æ›´æ–°åGallery: {len(clean_paths)} å¼ ")
    print(f"   âœ… ç´¢å¼•å¤§å°: {faiss_index.ntotal:,} patches")


# ==================== ä¸»å‡½æ•° ====================
if __name__ == '__main__':
    # é…ç½®
    config = Config()

    # âœ… éªŒè¯è·¯å¾„
    print("\n" + "=" * 60)
    print("ğŸ” éªŒè¯é…ç½®")
    print("=" * 60)
    print(f"âœ… åˆå§‹æ­£å¸¸å›¾è·¯å¾„: {config.INITIAL_NORMAL_DIR}")
    print(f"âœ… æ··åˆæ•°æ®è·¯å¾„: {config.MIXED_DIR}")
    print(f"âœ… æµ‹è¯•æ•°æ®è·¯å¾„: {config.TEST_DIR}")
    print(f"âœ… DINOv3ä»“åº“: {config.DINOV3_REPO_ROOT}")
    print(f"âœ… DINOv3æƒé‡: {config.DINOV3_WEIGHT_PATH}")
    print(f"âœ… Backbone: {config.BACKBONE}")
    print(f"âœ… è®¾å¤‡: {config.DEVICE}")

    # æ£€æŸ¥è·¯å¾„
    if not os.path.exists(config.INITIAL_NORMAL_DIR):
        raise FileNotFoundError(f"è®­ç»ƒæ•°æ®è·¯å¾„ä¸å­˜åœ¨: {config.INITIAL_NORMAL_DIR}")
    if not os.path.exists(config.DINOV3_REPO_ROOT):
        raise FileNotFoundError(f"DINOv3ä»“åº“ä¸å­˜åœ¨: {config.DINOV3_REPO_ROOT}")
    if not os.path.exists(config.DINOV3_WEIGHT_PATH):
        raise FileNotFoundError(f"DINOv3æƒé‡ä¸å­˜åœ¨: {config.DINOV3_WEIGHT_PATH}")

    pipeline = AnomalyDetectionPipeline(config)

    # è®­ç»ƒ
    feature_extractor, faiss_index, clean_paths = pipeline.run()

    # æ¨ç†
    if os.path.exists(config.TEST_DIR):  # âœ… ä½¿ç”¨config.TEST_DIR
        results = pipeline.inference(config.TEST_DIR, feature_extractor, faiss_index)

        # æ‰“å°Top 10
        print("\n" + "=" * 60)
        print("ğŸ” Top 10 æœ€å¯ç–‘å¼‚å¸¸")
        print("=" * 60)
        for r in results[:10]:
            status = 'ğŸ”´' if r['is_anomaly'] else 'ğŸŸ¢'
            print(f"{status} Rank {r['rank']}: {r['score']:.6f} - {os.path.basename(r['path'])}")
    else:
        print(f"âš ï¸  æµ‹è¯•è·¯å¾„ä¸å­˜åœ¨ï¼Œè·³è¿‡æ¨ç†")
