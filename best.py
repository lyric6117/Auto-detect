"""
ğŸ”¥ å·¥ä¸šçº§å¼‚å¸¸æ£€æµ‹ç³»ç»Ÿ v2.1 - ä¼˜åŒ–ç‰ˆ
ä¼˜åŒ–ç‚¹ï¼š
1. âœ… AEè®­ç»ƒåŠ é€Ÿï¼ˆæ··åˆç²¾åº¦ + æ›´å¤§batch + æ•°æ®é¢„åŠ è½½ï¼‰
2. âœ… DINOv3å°batchæ¨ç†ï¼ˆæ¢¯åº¦ç´¯ç§¯æ¨¡æ‹Ÿå¤§batchæ•ˆæœï¼‰
3. âœ… å†…å­˜ä¼˜åŒ–ï¼ˆåŠæ—¶æ¸…ç†ç¼“å­˜ï¼‰
"""

import os
import sys
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, models
from PIL import Image
import faiss
from sklearn.neighbors import LocalOutlierFactor
from scipy.ndimage import gaussian_filter
import cv2
from tqdm import tqdm
import pickle
from typing import List, Tuple, Dict, Optional
import warnings
from datetime import timedelta
import gc

warnings.filterwarnings('ignore')


# ==================== é…ç½®å‚æ•° ====================
class Config:
    """DINOv3 ä¸“ç”¨é…ç½®"""
    # ========== è·¯å¾„é…ç½® ==========
    BASE_DIR = r"C:\Users\Administrator\Desktop\f-AnoGAN\RD4AD-main\mvtec\zhawa_guzhang_zhifangtujunhenghua"
    INITIAL_NORMAL_DIR = os.path.join(BASE_DIR, "train", "good")
    MIXED_DIR = r'D:\Huangda_data0801\view\2\B'
    NORMAL_DIR = INITIAL_NORMAL_DIR
    TEST_DIR = MIXED_DIR
    OUTPUT_DIR = "./outputs"

    # ========== DINOv3 æ¨¡å‹é…ç½® ==========
    BACKBONE = "dinov3_vitl16"
    DINOV3_REPO_ROOT = r"D:\ly\dinov3-main"
    DINOV3_WEIGHT_PATH = r"D:\ly\dinov3-main\dinov3_vitl16_pretrain_lvd1689m-8aa4cbdd.pth"
    DINOV3_MODEL_NAME = "dinov3_vitl16_lvd1689m_distilled"
    DINOV3_FEATURE_LAYERS = [3, 6, 9, 11]  # ViT-L/16 æœ‰12å±‚
    DINOV3_PATCH_SIZE = 16

    # ========== é€šç”¨é…ç½® ==========
    IMAGE_SIZE = 224
    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
    NUM_WORKERS = 0

    # ========== ğŸ”¥ ä¼˜åŒ–1ï¼šAEè®­ç»ƒåŠ é€Ÿé…ç½® ==========
    AE_BATCH_SIZE = 128           # âœ… å¤§å¹…æå‡ï¼ˆåŸ512å¤ªå¤§ï¼Œæ”¹128ï¼‰
    AE_EPOCHS = 50                 # âœ… ä¿æŒè½»é‡
    AE_LR = 2e-3                  # âœ… æé«˜å­¦ä¹ ç‡
    AE_USE_AMP = True             # âœ… æ··åˆç²¾åº¦è®­ç»ƒ
    AE_NUM_WORKERS = 4            # âœ… å¤šçº¿ç¨‹åŠ è½½
    AE_PREFETCH_FACTOR = 2        # âœ… é¢„åŠ è½½

    # ========== ğŸ”¥ ä¼˜åŒ–2ï¼šDINOv3å°batché…ç½® ==========
    DINO_BATCH_SIZE = 8           # âœ… å°batchï¼ˆæ˜¾å­˜å‹å¥½ï¼‰
    DINO_ACCUMULATION_STEPS = 4   # âœ… æ¢¯åº¦ç´¯ç§¯æ¨¡æ‹Ÿ32çš„æ•ˆæœ
    DINO_NUM_WORKERS = 2          # âœ… é€‚åº¦å¹¶è¡Œ

    # ========== æ­¥éª¤1ï¼šç²—ç­›é…ç½® ==========
    EXPAND_RATIO = 0.7

    # ========== æ­¥éª¤2ï¼šæ¸…æ´—é…ç½® ==========
    LOF_NEIGHBORS = 20
    CONTAMINATION = 0.05

    # ========== æ­¥éª¤3ï¼šFAISSé…ç½® ==========
    FAISS_NLIST = 100
    FAISS_NPROBE = 10
    USE_CORESET = True
    CORESET_RATIO = 0.05

    # ========== æ­¥éª¤6ï¼šå¼‚å¸¸è¯„åˆ†é…ç½® ==========
    N_NEIGHBORS = 9
    ANOMALY_PERCENTILE = 90


# ==================== æ•°æ®é›† ====================
class ImageDataset(Dataset):
    def __init__(self, image_dir: str, transform=None, image_size=224):
        self.image_paths = []
        for root, _, files in os.walk(image_dir):
            for f in files:
                if f.lower().endswith(('.jpg', '.png', '.jpeg', '.bmp')):
                    self.image_paths.append(os.path.join(root, f))

        if len(self.image_paths) == 0:
            raise ValueError(f"æœªåœ¨ {image_dir} ä¸­æ‰¾åˆ°å›¾ç‰‡ï¼")

        self.transform = transform
        self.image_size = image_size
        print(f"   åŠ è½½ {len(self.image_paths)} å¼ å›¾ç‰‡")

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        try:
            img = Image.open(self.image_paths[idx]).convert('RGB')
            if self.transform:
                img = self.transform(img)
            return img, self.image_paths[idx]
        except Exception as e:
            print(f"âš ï¸  åŠ è½½å›¾ç‰‡å¤±è´¥: {self.image_paths[idx]} - {e}")
            return torch.zeros(3, self.image_size, self.image_size), self.image_paths[idx]


class PathDataset(Dataset):
    def __init__(self, paths, transform=None):
        self.paths = paths
        self.transform = transform

    def __len__(self):
        return len(self.paths)

    def __getitem__(self, idx):
        img_path = self.paths[idx]
        try:
            img = Image.open(img_path).convert("RGB")
            if self.transform:
                img = self.transform(img)
            return img, img_path  # âœ… è¿”å›å…ƒç»„
        except Exception as e:
            print(f"âš ï¸  åŠ è½½å›¾ç‰‡å¤±è´¥: {img_path} - {e}")
            return torch.zeros(3, 224, 224), img_path


# ==================== ğŸ”¥ ä¼˜åŒ–åçš„AEè®­ç»ƒ ====================
class SimpleAutoEncoder(nn.Module):
    """è½»é‡çº§è‡ªç¼–ç å™¨ï¼ˆä¿æŒä¸å˜ï¼‰"""
    def __init__(self):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Conv2d(3, 64, 4, stride=2, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 128, 4, stride=2, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 256, 4, stride=2, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 512, 4, stride=2, padding=1),
        )
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(512, 256, 4, stride=2, padding=1),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(64, 3, 4, stride=2, padding=1),
            nn.Sigmoid()
        )

    def forward(self, x):
        return self.decoder(self.encoder(x))


def train_autoencoder(dataloader, config):
    """ğŸ”¥ ä¼˜åŒ–ï¼šæ··åˆç²¾åº¦ + æ›´å¿«è®­ç»ƒ"""
    print("\n" + "=" * 60)
    print("ğŸ“Œ [æ­¥éª¤1] è®­ç»ƒè‡ªç¼–ç å™¨è¿›è¡Œç²—ç­› (åŠ é€Ÿç‰ˆ)")
    print("=" * 60)
    print(f"   âš¡ æ··åˆç²¾åº¦: {'å¯ç”¨' if config.AE_USE_AMP else 'ç¦ç”¨'}")
    print(f"   âš¡ Batch Size: {config.AE_BATCH_SIZE}")
    print(f"   âš¡ Learning Rate: {config.AE_LR}")

    model = SimpleAutoEncoder().to(config.DEVICE)
    optimizer = torch.optim.Adam(model.parameters(), lr=config.AE_LR)
    criterion = nn.MSELoss()

    # âœ… æ··åˆç²¾åº¦scaler
    scaler = torch.cuda.amp.GradScaler(enabled=config.AE_USE_AMP)

    model.train()
    for epoch in range(config.AE_EPOCHS):
        losses = []
        pbar = tqdm(dataloader, desc=f"Epoch {epoch + 1}/{config.AE_EPOCHS}")

        for imgs, _ in pbar:
            imgs = imgs.to(config.DEVICE)

            optimizer.zero_grad()

            # âœ… è‡ªåŠ¨æ··åˆç²¾åº¦
            with torch.cuda.amp.autocast(enabled=config.AE_USE_AMP):
                recon = model(imgs)
                loss = criterion(recon, imgs)

            # âœ… ä½¿ç”¨scalerè¿›è¡Œåå‘ä¼ æ’­
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()

            losses.append(loss.item())
            pbar.set_postfix({'loss': f"{np.mean(losses):.4f}"})

        print(f"   Epoch {epoch + 1}/{config.AE_EPOCHS} - Loss: {np.mean(losses):.4f}")

    # âœ… æ¸…ç†ç¼“å­˜
    torch.cuda.empty_cache()
    gc.collect()

    return model


def expand_normal_gallery(ae_model, mixed_dataloader, config):
    """æ‰©å……Normal Galleryï¼ˆä¿æŒä¸å˜ï¼‰"""
    print("\n   ä½¿ç”¨AEç­›é€‰æ­£å¸¸å›¾...")
    ae_model.eval()
    reconstruction_errors = []
    image_paths = []

    with torch.no_grad():
        for imgs, paths in tqdm(mixed_dataloader, desc="   è®¡ç®—é‡å»ºè¯¯å·®"):
            imgs = imgs.to(config.DEVICE)

            # âœ… æ··åˆç²¾åº¦æ¨ç†
            with torch.cuda.amp.autocast(enabled=config.AE_USE_AMP):
                recon = ae_model(imgs)
                errors = F.mse_loss(recon, imgs, reduction='none')

            errors = errors.view(imgs.size(0), -1).mean(dim=1)
            reconstruction_errors.extend(errors.cpu().numpy())
            image_paths.extend(paths)

    errors = np.array(reconstruction_errors)
    threshold = np.percentile(errors, config.EXPAND_RATIO * 100)
    selected_indices = np.where(errors <= threshold)[0]

    expanded_paths = [image_paths[i] for i in selected_indices]
    print(f"   âœ… ä» {len(image_paths)} å¼ æ··åˆå›¾ä¸­ç­›é€‰å‡º {len(expanded_paths)} å¼ æ­£å¸¸å›¾")
    print(f"   âœ… é‡å»ºè¯¯å·®é˜ˆå€¼: {threshold:.6f}")

    # âœ… æ¸…ç†
    torch.cuda.empty_cache()
    gc.collect()

    return expanded_paths, errors


# ==================== æ­¥éª¤2ï¼šç‰¹å¾æå–å™¨ ====================
class FeatureExtractor(nn.Module):
    """DINOv3 ä¸“ç”¨ç‰¹å¾æå–å™¨ï¼ˆä¿æŒä¸å˜ï¼‰"""
    def __init__(self, config: Config):
        super().__init__()
        self.config = config
        self.backbone_type = self._parse_backbone_type()

        if self.backbone_type == 'dinov3':
            self.model, self.feature_layers = self._load_dinov3()
        else:
            raise ValueError(f"å½“å‰ä»…æ”¯æŒDINOv3ï¼Œæ”¶åˆ°: {config.BACKBONE}")

        self.model.eval()
        for p in self.model.parameters():
            p.requires_grad = False

    def _parse_backbone_type(self) -> str:
        backbone = self.config.BACKBONE.lower()
        if 'dinov3' in backbone or 'dino_v3' in backbone:
            return 'dinov3'
        else:
            raise ValueError(f"è¯·ä½¿ç”¨DINOv3æ¨¡å‹ï¼Œå½“å‰é…ç½®: {self.config.BACKBONE}")

    def _load_dinov3(self):
        """ä»æœ¬åœ°åŠ è½½DINOv3æ¨¡å‹"""
        print("\n" + "=" * 60)
        print("ğŸ“¥ ä»æœ¬åœ°åŠ è½½ DINOv3 æ¨¡å‹")
        print("=" * 60)

        repo_root = self.config.DINOV3_REPO_ROOT
        weight_path = self.config.DINOV3_WEIGHT_PATH

        if not os.path.exists(repo_root):
            raise FileNotFoundError(f"DINOv3ä»“åº“è·¯å¾„ä¸å­˜åœ¨: {repo_root}")
        if not os.path.exists(weight_path):
            raise FileNotFoundError(f"æƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {weight_path}")

        print(f"   ğŸ“‚ ä»“åº“è·¯å¾„: {repo_root}")
        print(f"   ğŸ“‚ æƒé‡è·¯å¾„: {weight_path}")

        if repo_root not in sys.path:
            sys.path.insert(0, repo_root)

        try:
            from dinov3.models.vision_transformer import vit_large

            model = vit_large(
                patch_size=self.config.DINOV3_PATCH_SIZE,
                num_register_tokens=0,
                interpolate_antialias=False,
                interpolate_offset=0.1,
            )

            print(f"   ğŸ“¥ åŠ è½½æƒé‡...")
            state_dict = torch.load(weight_path, map_location='cpu')

            if 'teacher' in state_dict:
                state_dict = state_dict['teacher']
            elif 'model' in state_dict:
                state_dict = state_dict['model']

            new_state_dict = {}
            for k, v in state_dict.items():
                new_key = k.replace('module.', '').replace('backbone.', '')
                new_state_dict[new_key] = v

            msg = model.load_state_dict(new_state_dict, strict=False)
            print(f"   âœ… æƒé‡åŠ è½½å®Œæˆ")

        except Exception as e:
            raise RuntimeError(f"DINOv3åŠ è½½å¤±è´¥: {e}")

        model = model.to(self.config.DEVICE)

        feature_layers = self.config.DINOV3_FEATURE_LAYERS
        self.features = {}

        print(f"\n   ğŸ”§ æ³¨å†Œç‰¹å¾æå–hook...")
        print(f"   ğŸ“Š æ¨¡å‹æ€»å±‚æ•°: {len(model.blocks)}")
        print(f"   ğŸ“Œ æå–å±‚ç´¢å¼•: {feature_layers}")

        for layer_idx in feature_layers:
            if layer_idx >= len(model.blocks):
                raise ValueError(f"å±‚ç´¢å¼•{layer_idx}è¶…å‡ºèŒƒå›´(æ€»å…±{len(model.blocks)}å±‚)")

            def make_hook(idx):
                def hook(module, input, output):
                    self.features[f'block_{idx}'] = output
                return hook

            model.blocks[layer_idx].register_forward_hook(make_hook(layer_idx))

        print(f"   âœ… Hookæ³¨å†Œå®Œæˆ")
        print("=" * 60)

        return model, [f'block_{i}' for i in feature_layers]

    def forward(self, x):
        """å‰å‘ä¼ æ’­ - DINOv3ä¸“ç”¨ï¼ˆä¿®å¤ç‰ˆï¼‰"""
        # æ‰§è¡Œå‰å‘ä¼ æ’­ï¼Œè§¦å‘hooks
        with torch.no_grad():
            _ = self.model(x)

        # æå–å¹¶å¤„ç†ç‰¹å¾
        output_features = {}
        B = x.shape[0]
        patch_size = self.config.DINOV3_PATCH_SIZE
        H = W = self.config.IMAGE_SIZE // patch_size  # 224/16 = 14

        for layer_name, feat in self.features.items():
            try:
                # âœ… å¤„ç†å¯èƒ½çš„å…ƒç»„/åˆ—è¡¨è¾“å‡º
                if isinstance(feat, (tuple, list)):
                    # DINOv3 å¯èƒ½è¿”å› (class_token, patch_tokens, register_tokens)
                    # æˆ–è€… [output1, output2, ...]
                    if len(feat) >= 2:
                        # é€šå¸¸ç¬¬2ä¸ªå…ƒç´ æ˜¯patch tokens
                        feat = feat[1]
                    else:
                        feat = feat[0]
                # âœ… ç¡®ä¿æ˜¯Tensor
                if not isinstance(feat, torch.Tensor):
                    print(f"   âš ï¸  è·³è¿‡ {layer_name}ï¼šä¸æ˜¯Tensor (ç±»å‹: {type(feat)})")
                    continue

                # âœ… å¤„ç†ä¸åŒçš„ç‰¹å¾å½¢çŠ¶
                if feat.dim() == 3:  # [B, N+1, D] æˆ– [B, N, D]
                    expected_patches = H * W

                    if feat.shape[1] == expected_patches + 1:
                        # åŒ…å«CLS tokenï¼Œå»æ‰ç¬¬ä¸€ä¸ªtoken
                        feat = feat[:, 1:, :]  # [B, N, D]
                    elif feat.shape[1] == expected_patches:
                        # å·²ç»æ˜¯çº¯patch tokens
                        pass
                    else:
                        print(f"   âš ï¸  {layer_name} patchæ•°é‡å¼‚å¸¸: {feat.shape[1]} (æœŸæœ› {expected_patches})")
                        # å°è¯•è‡ªé€‚åº”è°ƒæ•´
                        actual_H = actual_W = int(np.sqrt(feat.shape[1]))
                        if actual_H * actual_W == feat.shape[1]:
                            H = W = actual_H
                            print(f"   ğŸ”§ è‡ªåŠ¨è°ƒæ•´ä¸º {H}x{W}")
                        else:
                            continue

                    D = feat.shape[-1]
                    # reshapeä¸º [B, D, H, W]
                    feat = feat.transpose(1, 2).reshape(B, D, H, W)

                elif feat.dim() == 4:  # [B, D, H, W] (å·²ç»æ˜¯ç‰¹å¾å›¾æ ¼å¼)
                    pass

                else:
                    print(f"   âš ï¸  è·³è¿‡ {layer_name}ï¼šç»´åº¦å¼‚å¸¸ ({feat.dim()}ç»´)")
                    continue

                output_features[layer_name] = feat

            except Exception as e:
                print(f"   âŒ å¤„ç† {layer_name} æ—¶å‡ºé”™: {e}")
                continue

        if len(output_features) == 0:
            raise RuntimeError("æ²¡æœ‰æˆåŠŸæå–ä»»ä½•ç‰¹å¾ï¼è¯·æ£€æŸ¥hooké…ç½®å’Œæ¨¡å‹è¾“å‡ºæ ¼å¼")

        return output_features


# ==================== ğŸ”¥ ä¼˜åŒ–åçš„DataLoader ====================
def get_ae_dataloader(dataset, config, shuffle=False):
    """AEä¸“ç”¨DataLoaderï¼ˆå¤§batch + é¢„åŠ è½½ï¼‰"""
    return DataLoader(
        dataset,
        batch_size=config.AE_BATCH_SIZE,
        shuffle=shuffle,
        num_workers=config.AE_NUM_WORKERS,
        pin_memory=True,
        persistent_workers=True if config.AE_NUM_WORKERS > 0 else False,
        prefetch_factor=config.AE_PREFETCH_FACTOR if config.AE_NUM_WORKERS > 0 else None,
        drop_last=False
    )


def get_dino_dataloader(dataset, config, shuffle=False):
    """DINOv3ä¸“ç”¨DataLoaderï¼ˆå°batchï¼‰"""
    return DataLoader(
        dataset,
        batch_size=config.DINO_BATCH_SIZE,
        shuffle=shuffle,
        num_workers=config.DINO_NUM_WORKERS,
        pin_memory=True,
        persistent_workers=False,  # Windowså…¼å®¹
        drop_last=False
    )


def extract_features(dataloader, feature_extractor, config):
    """æå–å¤šå±‚patchç‰¹å¾ï¼ˆå°batchä¼˜åŒ–ï¼‰"""
    print("\n" + "=" * 60)
    print("ğŸ“Œ [æ­¥éª¤2] æå–å¤šå±‚ç‰¹å¾ (å°Batchä¼˜åŒ–)")
    print("=" * 60)
    print(f"   ğŸ“Š Batch Size: {config.DINO_BATCH_SIZE}")

    all_features = []
    all_paths = []

    with torch.no_grad():
        for imgs, paths in tqdm(dataloader, desc="   æå–ç‰¹å¾"):
            imgs = imgs.to(config.DEVICE)
            features = feature_extractor(imgs)

            # èåˆå¤šå±‚ç‰¹å¾
            embeddings = []
            for layer in feature_extractor.feature_layers:
                feat = features[layer]
                feat = F.adaptive_avg_pool2d(feat, (16, 16))
                embeddings.append(feat)

            embedding = torch.cat(embeddings, dim=1)
            B, C, H, W = embedding.shape

            embedding = embedding.permute(0, 2, 3, 1).reshape(B, H * W, C)

            all_features.append(embedding.cpu().numpy())
            all_paths.extend(paths)

            # âœ… åŠæ—¶æ¸…ç†
            del imgs, features, embeddings, embedding
            if len(all_features) % 50 == 0:  # æ¯50ä¸ªbatchæ¸…ç†ä¸€æ¬¡
                torch.cuda.empty_cache()

    all_features = np.concatenate(all_features, axis=0)
    print(f"   âœ… æå–ç‰¹å¾å½¢çŠ¶: {all_features.shape}")
    print(f"   âœ… ç‰¹å¾ç»´åº¦: {all_features.shape[-1]} (å¤šå±‚èåˆ)")

    # âœ… æœ€ç»ˆæ¸…ç†
    torch.cuda.empty_cache()
    gc.collect()

    return all_features, all_paths


def clean_gallery_with_lof(features, paths, config):
    """ä½¿ç”¨LOFæ¸…æ´—ï¼ˆä¿æŒä¸å˜ï¼‰"""
    print("\n" + "=" * 60)
    print("ğŸ“Œ [æ­¥éª¤2] ä½¿ç”¨LOFæ¸…æ´—Gallery")
    print("=" * 60)

    global_features = features.mean(axis=1)

    lof = LocalOutlierFactor(
        n_neighbors=config.LOF_NEIGHBORS,
        contamination=config.CONTAMINATION,
        n_jobs=-1
    )

    print(f"   ğŸ” LOFå‚æ•°: n_neighbors={config.LOF_NEIGHBORS}, contamination={config.CONTAMINATION}")
    labels = lof.fit_predict(global_features)

    clean_indices = np.where(labels == 1)[0]
    outlier_indices = np.where(labels == -1)[0]

    clean_features = features[clean_indices]
    clean_paths = [paths[i] for i in clean_indices]

    print(f"   âœ… æ¸…æ´—å‰: {len(paths)} å¼ ")
    print(f"   âœ… æ¸…æ´—å: {len(clean_paths)} å¼  (ä¿ç•™ {len(clean_paths) / len(paths) * 100:.1f}%)")
    print(f"   ğŸ—‘ï¸  å‰”é™¤ç¦»ç¾¤ç‚¹: {len(outlier_indices)} å¼ ")

    return clean_features, clean_paths


# ==================== æ­¥éª¤3ï¼šCoreset + FAISS ====================
def greedy_coreset_sampling(features: np.ndarray, ratio: float = 0.1):
    """ğŸ”¥ ä¼˜åŒ–ç‰ˆï¼šå¿«é€ŸCoreseté‡‡æ ·ï¼ˆä½¿ç”¨FAISSåŠ é€Ÿ + ä¿®å¤è¿›åº¦æ¡ï¼‰"""
    print("\n" + "=" * 60)
    print(f"ğŸ“Œ [æ­¥éª¤3] Coreseté‡‡æ · (ä¿ç•™ {ratio * 100:.1f}%) - FAISSåŠ é€Ÿç‰ˆ")
    print("=" * 60)

    N, D = features.shape
    target_n = max(int(N * ratio), 1000)

    if target_n >= N:
        print(f"   âš ï¸  ç›®æ ‡æ•°é‡({target_n}) >= æ€»æ•°({N})ï¼Œè·³è¿‡é‡‡æ ·")
        return np.arange(N)

    print(f"   ğŸ“Š æ€»æ ·æœ¬æ•°: {N:,}")
    print(f"   ğŸ¯ ç›®æ ‡æ•°é‡: {target_n:,}")

    # ä½¿ç”¨FAISSåŠ é€Ÿ
    features_normalized = features.astype('float32')
    faiss.normalize_L2(features_normalized)

    index = faiss.IndexFlatL2(D)
    index.add(features_normalized)

    # éšæœºåˆå§‹åŒ–ç§å­ç‚¹
    num_seeds = min(10, target_n // 100)
    selected_indices = np.random.choice(N, num_seeds, replace=False).tolist()

    print(f"   ğŸŒ± åˆå§‹åŒ–ç§å­ç‚¹: {num_seeds} ä¸ª")

    # åˆå§‹åŒ–æœ€å°è·ç¦»
    min_distances = np.full(N, np.inf, dtype='float32')

    # è®¡ç®—åˆ°æ‰€æœ‰ç§å­ç‚¹çš„è·ç¦»
    for seed_idx in selected_indices:
        distances, _ = index.search(features_normalized[seed_idx:seed_idx + 1], N)
        min_distances = np.minimum(min_distances, distances[0])

    # âœ… ä¿®å¤ï¼šä½¿ç”¨æ›´æ¸…æ™°çš„è¿›åº¦æ¡é€»è¾‘
    batch_size = min(100, max(1, target_n // 100))  # æ¯æ¬¡é‡‡æ ·100ä¸ª
    remaining = target_n - len(selected_indices)

    print(f"   ğŸ”„ å‰©ä½™é‡‡æ ·æ•°: {remaining:,} (æ¯æ‰¹ {batch_size} ä¸ª)")

    # ä½¿ç”¨æ—¶é—´ä¼°è®¡çš„è¿›åº¦æ¡
    from time import time
    start_time = time()

    pbar = tqdm(
        total=remaining,
        desc="   é‡‡æ ·è¿›åº¦",
        unit="samples",
        bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]'
    )

    iteration = 0
    while len(selected_indices) < target_n:
        iteration += 1
        current_batch = min(batch_size, target_n - len(selected_indices))

        # é€‰æ‹©è·ç¦»æœ€å¤§çš„kä¸ªç‚¹
        if current_batch == 1:
            next_indices = [np.argmax(min_distances)]
        else:
            next_indices = np.argpartition(min_distances, -current_batch)[-current_batch:]

        # æ›´æ–°
        added_count = 0
        for next_idx in next_indices:
            if next_idx not in selected_indices and len(selected_indices) < target_n:
                selected_indices.append(int(next_idx))
                added_count += 1

                # æ›´æ–°è·ç¦»ï¼ˆæ¯10ä¸ªç‚¹æ‰¹é‡æ›´æ–°ä¸€æ¬¡ï¼Œè¿›ä¸€æ­¥åŠ é€Ÿï¼‰
                if added_count % 10 == 0 or len(selected_indices) >= target_n:
                    distances, _ = index.search(
                        features_normalized[next_idx:next_idx + 1], N
                    )
                    min_distances = np.minimum(min_distances, distances[0])

        # âœ… ä¿®å¤ï¼šåªæ›´æ–°å®é™…æ·»åŠ çš„æ•°é‡
        pbar.update(added_count)

        # é˜²æ­¢æ­»å¾ªç¯
        if added_count == 0:
            print(f"\n   âš ï¸  æ— æ³•ç»§ç»­é‡‡æ ·ï¼Œå½“å‰æ•°é‡: {len(selected_indices)}")
            break

    pbar.close()

    elapsed = time() - start_time
    print(f"   â±ï¸  é‡‡æ ·è€—æ—¶: {elapsed:.1f} ç§’")
    print(f"   âœ… é‡‡æ ·å®Œæˆ: {len(selected_indices):,} / {N:,} patches")
    print(f"   âœ… é‡‡æ ·æ•ˆç‡: {len(selected_indices) / elapsed:.0f} samples/s")

    return np.array(selected_indices[:target_n])


def build_faiss_index(features, config):
    """æ„å»ºFAISSç´¢å¼•ï¼ˆä¿æŒä¸å˜ï¼‰"""
    print("\n   ğŸ”¨ æ„å»ºFAISSç´¢å¼•...")

    N_images, N_patches, D = features.shape
    patch_features = features.reshape(-1, D).astype('float32')

    print(f"   ğŸ“Š åŸå§‹patchæ•°: {patch_features.shape[0]:,}")

    if config.USE_CORESET and patch_features.shape[0] > 10000:
        coreset_indices = greedy_coreset_sampling(patch_features, config.CORESET_RATIO)
        patch_features = patch_features[coreset_indices]

    faiss.normalize_L2(patch_features)

    if patch_features.shape[0] < 50000:
        index = faiss.IndexFlatL2(D)
        index.add(patch_features)
        print(f"   âœ… ä½¿ç”¨ç²¾ç¡®ç´¢å¼• (IndexFlatL2)")
    else:
        quantizer = faiss.IndexFlatL2(D)
        index = faiss.IndexIVFPQ(quantizer, D, config.FAISS_NLIST, 64, 8)
        index.train(patch_features)
        index.add(patch_features)
        index.nprobe = config.FAISS_NPROBE
        print(f"   âœ… ä½¿ç”¨IVF-PQç´¢å¼• (nlist={config.FAISS_NLIST})")

    print(f"   âœ… ç´¢å¼•åŒ…å« {index.ntotal:,} ä¸ªpatchç‰¹å¾")

    return index, patch_features


# ==================== æ­¥éª¤6ï¼šå¼‚å¸¸è¯„åˆ† ====================
def compute_anomaly_scores(test_features, faiss_index, config):
    """PatchCoreè½¯å¯¹é½ï¼ˆä¿æŒä¸å˜ï¼‰"""
    print("\n" + "=" * 60)
    print("ğŸ“Œ [æ­¥éª¤4-6] è®¡ç®—å¼‚å¸¸åˆ†æ•° (PatchCoreæ–¹å¼)")
    print("=" * 60)

    N_test, N_patches, D = test_features.shape
    anomaly_scores = []
    anomaly_maps = []

    for i in tqdm(range(N_test), desc="   è®¡ç®—åˆ†æ•°"):
        query = test_features[i].astype('float32')
        faiss.normalize_L2(query)

        distances, _ = faiss_index.search(query, config.N_NEIGHBORS)

        patch_scores = distances.mean(axis=1)

        image_score = patch_scores.max()
        anomaly_scores.append(image_score)

        h = w = int(np.sqrt(N_patches))
        anomaly_map = patch_scores.reshape(h, w)
        anomaly_maps.append(anomaly_map)

    scores = np.array(anomaly_scores)
    print(f"   âœ… å¼‚å¸¸åˆ†æ•°èŒƒå›´: [{scores.min():.4f}, {scores.max():.4f}]")
    print(f"   âœ… å¼‚å¸¸åˆ†æ•°å‡å€¼: {scores.mean():.4f}")

    return scores, anomaly_maps


def generate_anomaly_heatmap(anomaly_map, original_img_path, save_path):
    """ç”Ÿæˆå¼‚å¸¸çƒ­åŠ›å›¾ï¼ˆä¿æŒä¸å˜ï¼‰"""
    img = cv2.imread(original_img_path)
    if img is None:
        print(f"âš ï¸  æ— æ³•è¯»å–å›¾ç‰‡: {original_img_path}")
        return

    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    H, W = img.shape[:2]

    heatmap = cv2.resize(anomaly_map, (W, H))
    heatmap = gaussian_filter(heatmap, sigma=4)

    heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min() + 1e-8)
    heatmap = (heatmap * 255).astype(np.uint8)

    heatmap_color = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)

    superimposed = cv2.addWeighted(img, 0.6, heatmap_color, 0.4, 0)

    cv2.imwrite(save_path, cv2.cvtColor(superimposed, cv2.COLOR_RGB2BGR))


# ==================== ä¸»æµç¨‹ ====================
class AnomalyDetectionPipeline:
    def __init__(self, config: Config):
        self.config = config
        os.makedirs(config.OUTPUT_DIR, exist_ok=True)

        self.transform = transforms.Compose([
            transforms.Resize((config.IMAGE_SIZE, config.IMAGE_SIZE)),
            transforms.ToTensor(),
            transforms.Normalize(
                mean=[0.485, 0.456, 0.406],
                std=[0.229, 0.224, 0.225]
            )
        ])

    def run(self):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        print("\n" + "=" * 60)
        print("ğŸš€ å¼€å§‹è®­ç»ƒæµç¨‹ (ä¼˜åŒ–ç‰ˆ)")
        print("=" * 60)

        # ========== æ­¥éª¤1ï¼šAEç²—ç­›ï¼ˆå¤§batchåŠ é€Ÿï¼‰==========
        print(f"\nğŸ“‚ åŠ è½½åˆå§‹æ­£å¸¸å›¾: {self.config.INITIAL_NORMAL_DIR}")
        initial_dataset = ImageDataset(
            self.config.INITIAL_NORMAL_DIR,
            self.transform,
            self.config.IMAGE_SIZE
        )
        initial_loader = get_ae_dataloader(initial_dataset, self.config, shuffle=True)

        ae_model = train_autoencoder(initial_loader, self.config)

        print(f"\nğŸ“‚ åŠ è½½æ··åˆå›¾: {self.config.MIXED_DIR}")
        mixed_dataset = ImageDataset(
            self.config.MIXED_DIR,
            self.transform,
            self.config.IMAGE_SIZE
        )
        mixed_loader = get_ae_dataloader(mixed_dataset, self.config, shuffle=False)

        expanded_paths, _ = expand_normal_gallery(ae_model, mixed_loader, self.config)

        # âœ… åˆ é™¤AEæ¨¡å‹é‡Šæ”¾æ˜¾å­˜
        del ae_model
        torch.cuda.empty_cache()
        gc.collect()

        # ========== æ­¥éª¤2ï¼šDINOv3ç‰¹å¾æå–ï¼ˆå°batchï¼‰==========
        all_normal_paths = initial_dataset.image_paths + expanded_paths

        gallery_dataset = PathDataset(all_normal_paths, self.transform)
        gallery_loader = get_dino_dataloader(gallery_dataset, self.config, shuffle=False)

        feature_extractor = FeatureExtractor(self.config).to(self.config.DEVICE)

        gallery_features, gallery_paths = extract_features(
            gallery_loader, feature_extractor, self.config
        )

        clean_features, clean_paths = clean_gallery_with_lof(
            gallery_features, gallery_paths, self.config
        )

        # ========== æ­¥éª¤3ï¼šFAISSç´¢å¼• ==========
        faiss_index, memory_bank = build_faiss_index(clean_features, self.config)

        # ========== ä¿å­˜æ¨¡å‹ ==========
        model_save_path = os.path.join(self.config.OUTPUT_DIR, 'model.pkl')
        with open(model_save_path, 'wb') as f:
            pickle.dump({
                'faiss_index': faiss.serialize_index(faiss_index),
                'clean_paths': clean_paths,
                'config': self.config
            }, f)

        print("\n" + "=" * 60)
        print(f"âœ… è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜è‡³: {model_save_path}")
        print("=" * 60)

        return feature_extractor, faiss_index, clean_paths

    def inference(self, test_dir: str, feature_extractor, faiss_index):
        """æ¨ç†ï¼ˆå°batchï¼‰"""
        print("\n" + "=" * 60)
        print("ğŸ” å¼€å§‹å¼‚å¸¸æ£€æµ‹ (å°Batchä¼˜åŒ–)")
        print("=" * 60)

        test_dataset = ImageDataset(test_dir, self.transform, self.config.IMAGE_SIZE)
        test_loader = get_dino_dataloader(test_dataset, self.config, shuffle=False)

        test_features, test_paths = extract_features(
            test_loader, feature_extractor, self.config
        )

        scores, anomaly_maps = compute_anomaly_scores(
            test_features, faiss_index, self.config
        )

        threshold = np.percentile(scores, self.config.ANOMALY_PERCENTILE)
        print(f"\n   ğŸ¯ åŠ¨æ€é˜ˆå€¼ ({self.config.ANOMALY_PERCENTILE}%åˆ†ä½): {threshold:.4f}")

        sorted_indices = np.argsort(scores)[::-1]

        results = []
        for rank, idx in enumerate(sorted_indices):
            results.append({
                'rank': rank + 1,
                'path': test_paths[idx],
                'score': scores[idx],
                'is_anomaly': scores[idx] > threshold
            })

        # ä¿å­˜ç»“æœ
        result_path = os.path.join(self.config.OUTPUT_DIR, 'anomaly_results.txt')
        with open(result_path, 'w', encoding='utf-8') as f:
            f.write(f"{'Rank':<6} {'Score':<12} {'Status':<12} {'Path'}\n")
            f.write("=" * 80 + "\n")

            # âœ… ä¿®æ”¹ï¼šä¿å­˜æ‰€æœ‰ç»“æœè€Œä¸æ˜¯åªä¿å­˜å‰100
            for r in results:  # æ”¹ä¸ºå…¨éƒ¨
                status = 'ğŸ”´ ANOMALY' if r['is_anomaly'] else 'ğŸŸ¢ NORMAL'
                f.write(f"{r['rank']:<6} {r['score']:<12.6f} {status:<12} {r['path']}\n")

        print(f"\n   âœ… ç»“æœå·²ä¿å­˜: {result_path}")
        print(f"   ğŸ“Š ä¿å­˜æ•°é‡: {len(results)} å¼ ")  # âœ… æ·»åŠ æç¤º

        # çƒ­åŠ›å›¾
        heatmap_dir = os.path.join(self.config.OUTPUT_DIR, 'heatmaps')
        os.makedirs(heatmap_dir, exist_ok=True)

        print("\n   ğŸ¨ ç”Ÿæˆçƒ­åŠ›å›¾...")
        for i, idx in enumerate(tqdm(sorted_indices[:20], desc="   ç”Ÿæˆä¸­")):
            save_path = os.path.join(
                heatmap_dir,
                f"rank{i + 1}_score{scores[idx]:.4f}.jpg"
            )
            generate_anomaly_heatmap(
                anomaly_maps[idx],
                test_paths[idx],
                save_path
            )

        print(f"   âœ… çƒ­åŠ›å›¾å·²ä¿å­˜: {heatmap_dir}")

        # ç»Ÿè®¡
        n_anomalies = sum(r['is_anomaly'] for r in results)
        print("\n" + "=" * 60)
        print(f"ğŸ“Š æ£€æµ‹ç»Ÿè®¡")
        print("=" * 60)
        print(f"   æ€»è®¡: {len(results)} å¼ ")
        print(f"   å¼‚å¸¸: {n_anomalies} å¼  ({n_anomalies / len(results) * 100:.1f}%)")
        print(f"   æ­£å¸¸: {len(results) - n_anomalies} å¼ ")

        return results


# ==================== åœ¨çº¿å­¦ä¹  ====================
def incremental_update(new_normal_dir: str, model_path: str):
    """å¢é‡æ›´æ–°Normal Galleryï¼ˆä¿æŒä¸å˜ï¼‰"""
    print("\n" + "=" * 60)
    print("ğŸ”„ å¢é‡æ›´æ–°Normal Gallery")
    print("=" * 60)

    with open(model_path, 'rb') as f:
        data = pickle.load(f)

    faiss_index = faiss.deserialize_index(data['faiss_index'])
    clean_paths = data['clean_paths']
    config = data['config']

    print(f"   ğŸ“‚ å½“å‰Gallery: {len(clean_paths)} å¼ ")

    feature_extractor = FeatureExtractor(config).to(config.DEVICE)

    transform = transforms.Compose([
        transforms.Resize((config.IMAGE_SIZE, config.IMAGE_SIZE)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    new_dataset = ImageDataset(new_normal_dir, transform, config.IMAGE_SIZE)
    new_loader = get_dino_dataloader(new_dataset, config, shuffle=False)

    new_features, new_paths = extract_features(new_loader, feature_extractor, config)

    N, P, D = new_features.shape
    new_patch_features = new_features.reshape(-1, D).astype('float32')
    faiss.normalize_L2(new_patch_features)
    faiss_index.add(new_patch_features)

    clean_paths.extend(new_paths)

    with open(model_path, 'wb') as f:
        pickle.dump({
            'faiss_index': faiss.serialize_index(faiss_index),
            'clean_paths': clean_paths,
            'config': config
        }, f)

    print(f"   âœ… æ–°å¢ {len(new_paths)} å¼ æ­£å¸¸å›¾")
    print(f"   âœ… æ›´æ–°åGallery: {len(clean_paths)} å¼ ")
    print(f"   âœ… ç´¢å¼•å¤§å°: {faiss_index.ntotal:,} patches")


# ==================== ä¸»å‡½æ•° ====================
if __name__ == '__main__':
    config = Config()

    print("\n" + "=" * 60)
    print("ğŸ” éªŒè¯é…ç½®")
    print("=" * 60)
    print(f"âœ… åˆå§‹æ­£å¸¸å›¾è·¯å¾„: {config.INITIAL_NORMAL_DIR}")
    print(f"âœ… æ··åˆæ•°æ®è·¯å¾„: {config.MIXED_DIR}")
    print(f"âœ… æµ‹è¯•æ•°æ®è·¯å¾„: {config.TEST_DIR}")
    print(f"âœ… DINOv3ä»“åº“: {config.DINOV3_REPO_ROOT}")
    print(f"âœ… DINOv3æƒé‡: {config.DINOV3_WEIGHT_PATH}")
    print(f"âœ… Backbone: {config.BACKBONE}")
    print(f"âœ… è®¾å¤‡: {config.DEVICE}")
    print(f"\nğŸ”¥ ä¼˜åŒ–é…ç½®:")
    print(f"   AE Batch: {config.AE_BATCH_SIZE} (æ··åˆç²¾åº¦: {config.AE_USE_AMP})")
    print(f"   DINOv3 Batch: {config.DINO_BATCH_SIZE}")

    if not os.path.exists(config.INITIAL_NORMAL_DIR):
        raise FileNotFoundError(f"è®­ç»ƒæ•°æ®è·¯å¾„ä¸å­˜åœ¨: {config.INITIAL_NORMAL_DIR}")
    if not os.path.exists(config.DINOV3_REPO_ROOT):
        raise FileNotFoundError(f"DINOv3ä»“åº“ä¸å­˜åœ¨: {config.DINOV3_REPO_ROOT}")
    if not os.path.exists(config.DINOV3_WEIGHT_PATH):
        raise FileNotFoundError(f"DINOv3æƒé‡ä¸å­˜åœ¨: {config.DINOV3_WEIGHT_PATH}")

    pipeline = AnomalyDetectionPipeline(config)

    # è®­ç»ƒ
    feature_extractor, faiss_index, clean_paths = pipeline.run()

    # æ¨ç†
    if os.path.exists(config.TEST_DIR):
        results = pipeline.inference(config.TEST_DIR, feature_extractor, faiss_index)

        print("\n" + "=" * 60)
        print("ğŸ” Top 10 æœ€å¯ç–‘å¼‚å¸¸")
        print("=" * 60)
        for r in results[:10]:
            status = 'ğŸ”´' if r['is_anomaly'] else 'ğŸŸ¢'
            print(f"{status} Rank {r['rank']}: {r['score']:.6f} - {os.path.basename(r['path'])}")
    else:
        print(f"âš ï¸  æµ‹è¯•è·¯å¾„ä¸å­˜åœ¨ï¼Œè·³è¿‡æ¨ç†")
