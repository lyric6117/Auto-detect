"""Patch-Levelå¼‚å¸¸æ£€æµ‹ï¼ˆä¿®å¤ç‰ˆ - å…¼å®¹DINOv3ï¼‰"""

import os
import sys
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from pathlib import Path
from PIL import Image
from tqdm import tqdm
from sklearn.decomposition import IncrementalPCA
import matplotlib.pyplot as plt
import pandas as pd
import cv2
import datetime
from torchvision import transforms
from torch.utils.data import Dataset, DataLoader
import pickle
import hashlib
import warnings
from scipy.ndimage import gaussian_filter
warnings.filterwarnings('ignore')


# ==================== GPUé¢„å¤„ç†ï¼ˆä¸å˜ï¼‰====================
class ImageDatasetGPU(Dataset):
    """GPUåŠ é€Ÿçš„å›¾åƒåŠ è½½"""
    def __init__(self, image_paths, image_size=512):
        self.image_paths = image_paths
        self.image_size = image_size
        self.transform = transforms.Compose([
            transforms.Resize((image_size, image_size),
                            interpolation=transforms.InterpolationMode.BILINEAR),
            transforms.ToTensor(),
        ])

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        path = self.image_paths[idx]
        try:
            img = Image.open(path).convert('RGB')
            img_tensor = self.transform(img)
            return img_tensor, path, True
        except Exception as e:
            return torch.zeros(3, self.image_size, self.image_size), path, False


class GPUPreprocessor(nn.Module):
    """GPUä¸Šåšnormalize"""
    def __init__(self, image_size=512):
        super().__init__()
        self.image_size = image_size
        self.register_buffer('mean', torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))
        self.register_buffer('std', torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))

    def forward(self, x):
        return (x - self.mean) / self.std


class PrefetchLoader:
    """å¼‚æ­¥é¢„åŠ è½½åˆ°GPU"""
    def __init__(self, loader, device):
        self.loader = loader
        self.device = device
        self.stream = torch.cuda.Stream()

    def __iter__(self):
        first = True
        for next_data in self.loader:
            with torch.cuda.stream(self.stream):
                next_data = [d.cuda(non_blocking=True) if isinstance(d, torch.Tensor) else d
                            for d in next_data]
            if not first:
                yield current_data
            else:
                first = False
            torch.cuda.current_stream().wait_stream(self.stream)
            current_data = next_data
        yield current_data

    def __len__(self):
        return len(self.loader)


# ==================== ğŸ”¥ ä¿®å¤ç‰ˆ Patch-Levelå¼‚å¸¸æ£€æµ‹ ====================
class PatchLevelAnomalyDetector:
    """PaDiMé£æ ¼çš„Patch-Levelå¼‚å¸¸æ£€æµ‹ï¼ˆä¿®å¤DINOv3å…¼å®¹æ€§ï¼‰"""

    def __init__(self, config):
        self.config = config
        self.device = torch.device(config.DEVICE)

        print("=" * 80)
        print("ğŸ”¥ Patch-Levelå¼‚å¸¸æ£€æµ‹ç³»ç»Ÿï¼ˆPaDiMé£æ ¼ + GPUåŠ é€Ÿï¼‰")
        print("=" * 80)

        self.feature_layers = getattr(config, 'FEATURE_LAYERS', [8, 16, 23])

        # GPUé¢„å¤„ç†
        self.gpu_preprocessor = GPUPreprocessor(config.IMAGE_SIZE).to(self.device)
        self.gpu_preprocessor.eval()

        # ç¼“å­˜
        self.cache_dir = Path(getattr(config, 'CACHE_DIR', 'cache'))
        self.cache_dir.mkdir(exist_ok=True)

        # åŠ è½½DINOv3
        self.model = self._load_dinov3()

        # é«˜æ–¯åˆ†å¸ƒå‚æ•°
        self.patch_means = None
        self.patch_inv_covs = None
        self.pca = None

        # æ•°æ®
        self.all_patch_features = None
        self.all_paths = None

    def _load_dinov3(self):
        """åŠ è½½DINOv3"""
        print(f"\nğŸ“¥ åŠ è½½DINOv3æ¨¡å‹: {self.config.MODEL_NAME}")

        repo_root = self.config.DINOV3_REPO_ROOT
        sys.path.insert(0, repo_root)

        import dinov3.distributed as distributed
        from dinov3.configs import setup_config, DinoV3SetupArgs, setup_job
        from dinov3.models import build_model_for_eval

        config_path = os.path.join(
            repo_root, "dinov3", "configs", "train", f"{self.config.MODEL_NAME}.yaml"
        )

        output_dir = "./outputs/dinov3_tmp"
        os.makedirs(output_dir, exist_ok=True)

        setup_job(output_dir=output_dir, distributed_enabled=False, seed=42,
                 distributed_timeout=datetime.timedelta(minutes=30))

        setup_args = DinoV3SetupArgs(
            config_file=config_path,
            pretrained_weights=self.config.DINOV3_WEIGHT_PATH,
            output_dir=output_dir,
            opts=[]
        )

        cfg = setup_config(setup_args, strict_cfg=False)
        model = build_model_for_eval(config=cfg, pretrained_weights=self.config.DINOV3_WEIGHT_PATH)

        model.eval().to(self.device)
        for p in model.parameters():
            p.requires_grad = False

        # ğŸ”¥ æ£€æŸ¥æ¨¡å‹ç»“æ„
        print(f"   æ¨¡å‹å±‚æ•°: {len(model.blocks)} layers")
        print(f"   æå–å±‚: {self.feature_layers}")

        # éªŒè¯å±‚ç´¢å¼•
        max_layer = max(self.feature_layers)
        if max_layer >= len(model.blocks):
            raise ValueError(f"FEATURE_LAYERSåŒ…å«æ— æ•ˆå±‚ç´¢å¼• {max_layer}ï¼Œæ¨¡å‹åªæœ‰ {len(model.blocks)} å±‚ï¼ˆ0-{len(model.blocks)-1}ï¼‰")

        print(f"âœ… DINOv3åŠ è½½å®Œæˆ")
        return model

    def _get_image_paths(self, directory):
        """è·å–å›¾åƒè·¯å¾„"""
        extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.JPG', '.JPEG', '.PNG', '.BMP')
        paths = []
        directory = Path(directory)
        for ext in extensions:
            paths.extend(directory.rglob(f"*{ext}"))
        return sorted([str(p) for p in set(paths)])

    @torch.no_grad()
    def extract_multiscale_patch_features(self, image_dir=None, use_cache=True):
        """æå–å¤šå°ºåº¦Patchç‰¹å¾"""
        if image_dir is None:
            image_dir = self.config.IMAGE_DIR

        print("\n" + "="*80)
        print("ğŸ” æå–å¤šå°ºåº¦Patchç‰¹å¾ï¼ˆMulti-scale Patch Tokensï¼‰")
        print("="*80)

        # ç¼“å­˜æ£€æŸ¥
        cache_key = hashlib.md5(
            f"{image_dir}_{self.config.MODEL_NAME}_{self.feature_layers}".encode()
        ).hexdigest()[:16]
        cache_file = self.cache_dir / f"patch_features_{cache_key}.pkl"

        if use_cache and cache_file.exists():
            print(f"ğŸ“¦ åŠ è½½ç¼“å­˜: {cache_file}")
            with open(cache_file, 'rb') as f:
                cache_data = pickle.load(f)
            self.all_patch_features = cache_data['patch_features']
            self.all_paths = cache_data['paths']
            print(f"âœ… ç¼“å­˜åŠ è½½å®Œæˆ: {self.all_patch_features.shape}")
            return self.all_patch_features, self.all_paths

        # è·å–å›¾åƒ
        image_paths = self._get_image_paths(image_dir)
        print(f"ğŸ“‚ æ‰¾åˆ°å›¾åƒ: {len(image_paths)} å¼ ")
        print(f"ğŸ¯ æå–å±‚: {self.feature_layers}")

        # DataLoader
        dataset = ImageDatasetGPU(image_paths, self.config.IMAGE_SIZE)
        dataloader = DataLoader(
            dataset, batch_size=self.config.BATCH_SIZE, shuffle=False,
            num_workers=self.config.NUM_WORKERS, pin_memory=True,
            prefetch_factor=4, persistent_workers=True if self.config.NUM_WORKERS > 0 else False
        )
        dataloader = PrefetchLoader(dataloader, self.device)

        # æå–ç‰¹å¾
        all_patch_feats = []
        valid_paths = []

        import time
        start_time = time.time()

        for batch_tensors, batch_paths, batch_valid in tqdm(dataloader, desc="æå–Patchç‰¹å¾"):
            valid_mask = batch_valid.cpu().numpy() if isinstance(batch_valid, torch.Tensor) else np.array(batch_valid)
            if not valid_mask.any():
                continue

            # GPUé¢„å¤„ç†
            batch_tensors = batch_tensors[valid_mask].cuda(non_blocking=True)
            batch_tensors = self.gpu_preprocessor(batch_tensors)
            batch_paths_valid = [p for p, v in zip(batch_paths, valid_mask) if v]

            # ğŸ”¥ æå–å¤šå±‚patch tokensï¼ˆä¿®å¤ç‰ˆï¼‰
            patch_feats = self._extract_patch_features_from_layers(batch_tensors)  # [B, H, W, D]

            all_patch_feats.append(patch_feats.cpu().numpy())
            valid_paths.extend(batch_paths_valid)

        elapsed = time.time() - start_time
        print(f"\nâ±ï¸  æå–è€—æ—¶: {elapsed:.1f}ç§’")
        print(f"ğŸ’¾ æ˜¾å­˜å³°å€¼: {torch.cuda.max_memory_allocated()/1024**3:.2f}GB")

        # åˆå¹¶
        patch_features = np.concatenate(all_patch_feats, axis=0)
        self.all_patch_features = patch_features
        self.all_paths = valid_paths

        print(f"âœ… Patchç‰¹å¾æå–å®Œæˆ: {patch_features.shape}")
        print(f"   - å›¾åƒæ•°: {patch_features.shape[0]}")
        print(f"   - Patchç½‘æ ¼: {patch_features.shape[1]}Ã—{patch_features.shape[2]}")
        print(f"   - ç‰¹å¾ç»´åº¦: {patch_features.shape[3]}")

        # PCAé™ç»´
        if getattr(self.config, 'USE_PCA', True):
            patch_features = self._apply_pca_to_patches(patch_features)
            self.all_patch_features = patch_features

        # ä¿å­˜ç¼“å­˜
        if use_cache:
            print(f"ğŸ’¾ ä¿å­˜ç¼“å­˜...")
            with open(cache_file, 'wb') as f:
                pickle.dump({'patch_features': patch_features, 'paths': valid_paths}, f)

        return patch_features, valid_paths

    def _extract_patch_features_from_layers(self, images):
        """
        ğŸ”¥ ä¿®å¤ç‰ˆï¼šä»å¤šä¸ªTransformerå±‚æå–patch tokens

        Args:
            images: [B, 3, H, W]

        Returns:
            patch_feats: [B, h, w, D]
        """
        B = images.size(0)

        # ğŸ”¥ å­˜å‚¨ä¸­é—´ç‰¹å¾
        intermediate_features = {}

        def hook_fn(name):
            def hook(module, input, output):
                # âœ… å¤„ç†å¯èƒ½çš„tuple/listè¾“å‡º
                if isinstance(output, (tuple, list)):
                    # å–ç¬¬ä¸€ä¸ªå…ƒç´ ï¼ˆé€šå¸¸æ˜¯ä¸»è¾“å‡ºï¼‰
                    intermediate_features[name] = output[0]
                else:
                    intermediate_features[name] = output
            return hook

        # æ³¨å†Œhooks
        hooks = []
        for layer_idx in self.feature_layers:
            if hasattr(self.model, 'blocks'):
                hook = self.model.blocks[layer_idx].register_forward_hook(
                    hook_fn(f'layer_{layer_idx}')
                )
                hooks.append(hook)
            else:
                raise AttributeError("æ¨¡å‹æ²¡æœ‰ 'blocks' å±æ€§ï¼Œè¯·æ£€æŸ¥DINOv3ç‰ˆæœ¬")

        # Forward pass
        try:
            _ = self.model.forward_features(images)
        except Exception as e:
            print(f"âŒ forward_featureså¤±è´¥: {e}")
            # ç§»é™¤hooks
            for hook in hooks:
                hook.remove()
            raise

        # ç§»é™¤hooks
        for hook in hooks:
            hook.remove()

        # ğŸ”¥ æå–patch tokensï¼ˆä¿®å¤ç‰ˆï¼‰
        patch_tokens_list = []

        for layer_idx in self.feature_layers:
            key = f'layer_{layer_idx}'
            if key not in intermediate_features:
                raise RuntimeError(f"æœªæ•è·åˆ° layer {layer_idx} çš„ç‰¹å¾ï¼Œå¯èƒ½hookå¤±è´¥")

            layer_out = intermediate_features[key]

            # âœ… ç¡®ä¿æ˜¯tensor
            if not isinstance(layer_out, torch.Tensor):
                print(f"âš ï¸  Layer {layer_idx} è¾“å‡ºç±»å‹: {type(layer_out)}")
                if isinstance(layer_out, (tuple, list)):
                    layer_out = layer_out[0]
                else:
                    raise TypeError(f"æ— æ³•å¤„ç†çš„è¾“å‡ºç±»å‹: {type(layer_out)}")

            # âœ… æ£€æŸ¥å½¢çŠ¶
            if layer_out.dim() != 3:
                raise ValueError(f"Layer {layer_idx} è¾“å‡ºå½¢çŠ¶å¼‚å¸¸: {layer_out.shape}ï¼ŒæœŸæœ› [B, N, D]")

            # å»æ‰cls tokenï¼ˆå‡è®¾ç¬¬0ä¸ªæ˜¯cls tokenï¼‰
            # DINOv3æ ¼å¼: [B, num_patches+1, embed_dim]
            patch_tokens = layer_out[:, 1:, :]  # [B, N, D]

            # âœ… L2 normalizeï¼ˆæ¯å±‚å•ç‹¬å½’ä¸€åŒ–ï¼‰
            patch_tokens = F.normalize(patch_tokens, dim=-1)

            patch_tokens_list.append(patch_tokens)

        # æ‹¼æ¥å¤šå±‚ç‰¹å¾
        patch_tokens = torch.cat(patch_tokens_list, dim=-1)  # [B, N, D*num_layers]

        # Reshapeåˆ°ç©ºé—´ç½‘æ ¼
        B, N, D = patch_tokens.shape
        h = int(np.floor(np.sqrt(N)))
        w = int(np.ceil(N / h))
        # å¦‚æœ h*w > Nï¼Œéœ€è¦è¡¥é›¶ patch
        if h * w > N:
            padding = torch.zeros(B, h * w - N, D, device=patch_tokens.device)
            patch_tokens = torch.cat([patch_tokens, padding], dim=1)
        patch_tokens = patch_tokens.reshape(B, h, w, D)

        return patch_tokens

    def _apply_pca_to_patches(self, patch_features):
        """å¯¹patchç‰¹å¾åšPCAé™ç»´"""
        N, H, W, D = patch_features.shape
        pca_dim = min(self.config.PCA_DIM, D, N*H*W - 1)

        print(f"ğŸ“‰ Patchç‰¹å¾PCA: {D} â†’ {pca_dim}")

        # Flatten
        X = patch_features.reshape(-1, D)

        # IncrementalPCA
        self.pca = IncrementalPCA(n_components=pca_dim, batch_size=10000)
        X_reduced = self.pca.fit_transform(X)

        # Reshape
        patch_features_reduced = X_reduced.reshape(N, H, W, pca_dim)

        print(f"âœ… PCAå®Œæˆ: {patch_features_reduced.shape}")
        return patch_features_reduced

    def fit_gaussian_distribution(self):
        """å»ºç«‹Patch-Levelé«˜æ–¯åˆ†å¸ƒ"""
        print("\n" + "="*80)
        print("ğŸ“Š å»ºç«‹Patch-Levelé«˜æ–¯åˆ†å¸ƒï¼ˆNormal Distribution Modelingï¼‰")
        print("="*80)

        N, H, W, D = self.all_patch_features.shape

        print(f"æ•°æ®å½¢çŠ¶: {self.all_patch_features.shape}")

        # è®¡ç®—å‡å€¼å’Œåæ–¹å·®
        self.patch_means = np.zeros((H, W, D), dtype=np.float32)
        self.patch_inv_covs = np.zeros((H, W, D, D), dtype=np.float32)

        cov_reg = getattr(self.config, 'COV_REGULARIZATION', 1e-3)

        print(f"åæ–¹å·®æ­£åˆ™åŒ–: {cov_reg}")
        print("\nè®¡ç®—ä¸­...")

        for i in tqdm(range(H), desc="ç©ºé—´ä½ç½®"):
            for j in range(W):
                features_at_pos = self.all_patch_features[:, i, j, :]  # [N, D]

                # å‡å€¼
                mean = features_at_pos.mean(axis=0)
                self.patch_means[i, j] = mean

                # åæ–¹å·®
                cov = np.cov(features_at_pos, rowvar=False)
                cov += np.eye(D) * cov_reg

                # æ±‚é€†
                try:
                    inv_cov = np.linalg.inv(cov)
                except:
                    inv_cov = np.eye(D)

                self.patch_inv_covs[i, j] = inv_cov

        print(f"âœ… é«˜æ–¯åˆ†å¸ƒå»ºç«‹å®Œæˆï¼")

    def compute_anomaly_maps(self, patch_features=None):
        """è®¡ç®—å¼‚å¸¸å›¾"""
        if patch_features is None:
            patch_features = self.all_patch_features

        print("\n" + "="*80)
        print("ğŸ’¯ è®¡ç®—Patch-Levelå¼‚å¸¸åˆ†æ•°ï¼ˆMahalanobis Distanceï¼‰")
        print("="*80)

        N, H, W, D = patch_features.shape
        anomaly_maps = np.zeros((N, H, W), dtype=np.float32)

        for n in tqdm(range(N), desc="è®¡ç®—å¼‚å¸¸åˆ†æ•°"):
            for i in range(H):
                for j in range(W):
                    feat = patch_features[n, i, j]
                    mean = self.patch_means[i, j]
                    inv_cov = self.patch_inv_covs[i, j]

                    delta = feat - mean
                    score = np.sqrt(delta @ inv_cov @ delta)

                    anomaly_maps[n, i, j] = score

        # å›¾åƒçº§åˆ†æ•°
        k = getattr(self.config, 'TOP_K_PATCHES', 10)
        image_scores = np.zeros(N, dtype=np.float32)

        for n in range(N):
            patch_scores = anomaly_maps[n].flatten()
            top_k_scores = np.partition(patch_scores, -k)[-k:]
            image_scores[n] = top_k_scores.mean()

        print(f"âœ… å¼‚å¸¸åˆ†æ•°è®¡ç®—å®Œæˆï¼")
        print(f"   - åˆ†æ•°èŒƒå›´: [{image_scores.min():.4f}, {image_scores.max():.4f}]")

        return anomaly_maps, image_scores

    def select_samples_for_annotation(self, image_scores):
        """ç­›é€‰æ ·æœ¬"""
        if self.config.USE_TOP_K:
            n_select = self.config.TOP_K
        else:
            n_select = int(len(image_scores) * self.config.TOP_PERCENT)

        selected_indices = np.argsort(image_scores)[-n_select:][::-1]
        return selected_indices

    def visualize_anomaly_maps(self, anomaly_maps, selected_indices, output_dir):
        """å¯è§†åŒ–å¼‚å¸¸çƒ­åŠ›å›¾"""
        print("\n" + "="*80)
        print("ğŸ¨ ç”Ÿæˆå¼‚å¸¸çƒ­åŠ›å›¾å¯è§†åŒ–")
        print("="*80)

        viz_dir = Path(output_dir) / 'anomaly_heatmaps'
        viz_dir.mkdir(exist_ok=True)

        viz_k = min(getattr(self.config, 'VIZ_TOP_K', 50), len(selected_indices))
        sigma = getattr(self.config, 'HEATMAP_SIGMA', 4)
        alpha = getattr(self.config, 'HEATMAP_ALPHA', 0.5)

        print(f"ç”ŸæˆTop-{viz_k}å¼‚å¸¸æ ·æœ¬çš„çƒ­åŠ›å›¾...")

        for idx in tqdm(selected_indices[:viz_k], desc="ç”Ÿæˆçƒ­åŠ›å›¾"):
            img_path = self.all_paths[idx]
            anomaly_map = anomaly_maps[idx]

            try:
                img_orig = cv2.imread(img_path)
                img_orig = cv2.cvtColor(img_orig, cv2.COLOR_BGR2RGB)
                h_orig, w_orig = img_orig.shape[:2]
            except:
                continue

            # ä¸Šé‡‡æ ·
            anomaly_map_resized = cv2.resize(anomaly_map, (w_orig, h_orig),
                                            interpolation=cv2.INTER_CUBIC)
            anomaly_map_smooth = gaussian_filter(anomaly_map_resized, sigma=sigma)

            # å½’ä¸€åŒ–
            anomaly_map_norm = (anomaly_map_smooth - anomaly_map_smooth.min()) / \
                              (anomaly_map_smooth.max() - anomaly_map_smooth.min() + 1e-8)

            # çƒ­åŠ›å›¾
            heatmap = cv2.applyColorMap(
                (anomaly_map_norm * 255).astype(np.uint8),
                cv2.COLORMAP_JET
            )
            heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)

            # å åŠ 
            overlay = (img_orig * (1 - alpha) + heatmap * alpha).astype(np.uint8)

            # ä¿å­˜
            fig, axes = plt.subplots(1, 3, figsize=(15, 5))

            axes[0].imshow(img_orig)
            axes[0].set_title('Original', fontsize=12)
            axes[0].axis('off')

            axes[1].imshow(anomaly_map_norm, cmap='jet')
            axes[1].set_title('Anomaly Map', fontsize=12)
            axes[1].axis('off')

            axes[2].imshow(overlay)
            axes[2].set_title('Overlay', fontsize=12)
            axes[2].axis('off')

            plt.suptitle(f'{Path(img_path).name}\nScore: {anomaly_map.max():.4f}',
                        fontsize=10)
            plt.tight_layout()

            save_path = viz_dir / f'{Path(img_path).stem}_heatmap.png'
            plt.savefig(save_path, dpi=150, bbox_inches='tight')
            plt.close()

        print(f"âœ… çƒ­åŠ›å›¾å·²ä¿å­˜åˆ°: {viz_dir}")

    def save_results(self, anomaly_maps, image_scores, selected_indices, output_dir):
        """ä¿å­˜ç»“æœ"""
        output_dir = Path(output_dir)
        output_dir.mkdir(exist_ok=True)

        print("\n" + "="*80)
        print("ğŸ’¾ ä¿å­˜ç»“æœ")
        print("="*80)

        # CSV
        df = pd.DataFrame({
            'image_path': self.all_paths,
            'image_name': [Path(p).name for p in self.all_paths],
            'anomaly_score': image_scores,
            'max_patch_score': anomaly_maps.max(axis=(1, 2)),
            'mean_patch_score': anomaly_maps.mean(axis=(1, 2)),
        })

        df['selected_for_annotation'] = 0
        df.loc[selected_indices, 'selected_for_annotation'] = 1
        df = df.sort_values('anomaly_score', ascending=False)

        csv_all = output_dir / 'all_images_with_scores.csv'
        df.to_csv(csv_all, index=False, encoding='utf-8-sig')
        print(f"ğŸ“„ å®Œæ•´ç»“æœ: {csv_all}")

        df_selected = df[df['selected_for_annotation'] == 1].copy()
        csv_selected = output_dir / 'samples_for_annotation.csv'
        df_selected.to_csv(csv_selected, index=False, encoding='utf-8-sig')
        print(f"ğŸ“„ å¾…æ ‡æ³¨: {csv_selected} ({len(df_selected)} ä¸ª)")

        # å¯è§†åŒ–
        self.visualize_anomaly_maps(anomaly_maps, selected_indices, output_dir)

        print(f"\nğŸ“ æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {output_dir.absolute()}")
        return csv_selected

    def run_pipeline(self, image_dir=None):
        """å®Œæ•´æµç¨‹"""
        print("\n" + "="*80)
        print("ğŸš€ å¼€å§‹Patch-Levelå¼‚å¸¸æ£€æµ‹Pipeline")
        print("="*80)

        # 1. æå–ç‰¹å¾
        patch_features, paths = self.extract_multiscale_patch_features(image_dir)

        # 2. å»ºç«‹åˆ†å¸ƒ
        self.fit_gaussian_distribution()

        # 3. è®¡ç®—å¼‚å¸¸
        anomaly_maps, image_scores = self.compute_anomaly_maps()

        # 4. ç­›é€‰
        selected_indices = self.select_samples_for_annotation(image_scores)
        print(f"\nğŸ¯ ç­›é€‰å‡º {len(selected_indices)} ä¸ªå¼‚å¸¸æ ·æœ¬")

        # 5. ä¿å­˜
        csv_file = self.save_results(
            anomaly_maps, image_scores, selected_indices,
            self.config.OUTPUT_DIR
        )

        print("\n" + "="*80)
        print("âœ… Pipelineå®Œæˆï¼")
        print("="*80)

        return csv_file


# ==================== ğŸš€ GPUåŠ é€Ÿç‰ˆæœ¬ ====================

class PatchLevelAnomalyDetectorFast(PatchLevelAnomalyDetector):
    """GPUå‘é‡åŒ–åŠ é€Ÿç‰ˆï¼ˆ10-100x fasterï¼‰"""

    def fit_gaussian_distribution(self):
        """ğŸš€ GPUæ‰¹é‡è®¡ç®—é«˜æ–¯åˆ†å¸ƒå‚æ•°"""
        print("\n" + "=" * 80)
        print("ğŸ“Š å»ºç«‹Patch-Levelé«˜æ–¯åˆ†å¸ƒï¼ˆGPUå‘é‡åŒ–åŠ é€Ÿï¼‰")
        print("=" * 80)

        N, H, W, D = self.all_patch_features.shape
        print(f"æ•°æ®å½¢çŠ¶: {self.all_patch_features.shape}")

        # è½¬GPU
        features_gpu = torch.from_numpy(self.all_patch_features).to(self.device)  # [N, H, W, D]
        features_gpu = features_gpu.permute(1, 2, 0, 3)  # [H, W, N, D]

        cov_reg = getattr(self.config, 'COV_REGULARIZATION', 1e-3)
        print(f"åæ–¹å·®æ­£åˆ™åŒ–: {cov_reg}")

        # âœ… æ‰¹é‡è®¡ç®—å‡å€¼ï¼ˆå‘é‡åŒ–ï¼‰
        patch_means = features_gpu.mean(dim=2)  # [H, W, D]

        # âœ… æ‰¹é‡è®¡ç®—åæ–¹å·®å’Œé€†çŸ©é˜µï¼ˆGPUåŠ é€Ÿï¼‰
        patch_inv_covs = torch.zeros(H, W, D, D, device=self.device)

        print("è®¡ç®—åæ–¹å·®çŸ©é˜µ...")
        for i in tqdm(range(H), desc="ç©ºé—´ä½ç½®ï¼ˆGPUåŠ é€Ÿï¼‰"):
            for j in range(W):
                X = features_gpu[i, j]  # [N, D]

                # ä¸­å¿ƒåŒ–
                X_centered = X - patch_means[i, j]  # [N, D]

                # åæ–¹å·®çŸ©é˜µ: C = X^T @ X / (N-1)
                cov = (X_centered.T @ X_centered) / (N - 1)  # [D, D]
                cov += torch.eye(D, device=self.device) * cov_reg

                # æ±‚é€†
                try:
                    inv_cov = torch.linalg.inv(cov)
                except:
                    inv_cov = torch.eye(D, device=self.device)

                patch_inv_covs[i, j] = inv_cov

        # ä¿å­˜åˆ°CPU
        self.patch_means = patch_means.cpu().numpy()
        self.patch_inv_covs = patch_inv_covs.cpu().numpy()

        print(f"âœ… é«˜æ–¯åˆ†å¸ƒå»ºç«‹å®Œæˆï¼ˆGPUåŠ é€Ÿï¼‰ï¼")

    def compute_anomaly_maps(self, patch_features=None):
        """ğŸš€ GPUæ‰¹é‡è®¡ç®—å¼‚å¸¸å›¾ï¼ˆå®Œå…¨å‘é‡åŒ–ï¼‰"""
        if patch_features is None:
            patch_features = self.all_patch_features

        print("\n" + "=" * 80)
        print("ğŸ’¯ è®¡ç®—å¼‚å¸¸åˆ†æ•°ï¼ˆGPUå‘é‡åŒ–ï¼Œæ— å¾ªç¯ï¼‰")
        print("=" * 80)

        N, H, W, D = patch_features.shape

        # è½¬GPU
        features_gpu = torch.from_numpy(patch_features).to(self.device)  # [N, H, W, D]
        means_gpu = torch.from_numpy(self.patch_means).to(self.device)  # [H, W, D]
        inv_covs_gpu = torch.from_numpy(self.patch_inv_covs).to(self.device)  # [H, W, D, D]

        # âœ… å®Œå…¨å‘é‡åŒ–è®¡ç®—Mahalanobisè·ç¦»
        print("è®¡ç®—Mahalanobisè·ç¦»ï¼ˆå‘é‡åŒ–ï¼‰...")

        # å±•å¼€ä¸º [N*H*W, D]
        features_flat = features_gpu.reshape(N * H * W, D)
        means_flat = means_gpu.unsqueeze(0).expand(N, -1, -1, -1).reshape(N * H * W, D)
        inv_covs_flat = inv_covs_gpu.unsqueeze(0).expand(N, -1, -1, -1, -1).reshape(N * H * W, D, D)

        # delta = x - mu
        delta = features_flat - means_flat  # [N*H*W, D]

        # Mahalanobis: sqrt(delta^T @ inv_cov @ delta)
        # æ‰¹é‡çŸ©é˜µä¹˜æ³•
        delta_cov = torch.bmm(delta.unsqueeze(1), inv_covs_flat)  # [N*H*W, 1, D]
        mahal_sq = torch.bmm(delta_cov, delta.unsqueeze(2)).squeeze()  # [N*H*W]
        mahal_dist = torch.sqrt(torch.clamp(mahal_sq, min=0))  # [N*H*W]

        # Reshapeå› [N, H, W]
        anomaly_maps = mahal_dist.reshape(N, H, W).cpu().numpy()

        # âœ… å›¾åƒçº§åˆ†æ•°ï¼ˆTop-K patchesï¼‰
        k = getattr(self.config, 'TOP_K_PATCHES', 10)
        anomaly_maps_flat = anomaly_maps.reshape(N, -1)
        top_k_scores = np.partition(anomaly_maps_flat, -k, axis=1)[:, -k:]
        image_scores = top_k_scores.mean(axis=1)

        print(f"âœ… å¼‚å¸¸åˆ†æ•°è®¡ç®—å®Œæˆï¼ˆGPUåŠ é€Ÿï¼‰ï¼")
        print(f"   - åˆ†æ•°èŒƒå›´: [{image_scores.min():.4f}, {image_scores.max():.4f}]")

        return anomaly_maps, image_scores

    def visualize_anomaly_maps(self, anomaly_maps, selected_indices, output_dir):
        """ğŸš€ å¤šè¿›ç¨‹å¹¶è¡Œç”Ÿæˆçƒ­åŠ›å›¾"""
        print("\n" + "=" * 80)
        print("ğŸ¨ ç”Ÿæˆå¼‚å¸¸çƒ­åŠ›å›¾ï¼ˆå¤šè¿›ç¨‹åŠ é€Ÿï¼‰")
        print("=" * 80)

        viz_dir = Path(output_dir) / 'anomaly_heatmaps'
        viz_dir.mkdir(exist_ok=True)

        viz_k = min(getattr(self.config, 'VIZ_TOP_K', 50), len(selected_indices))
        sigma = getattr(self.config, 'HEATMAP_SIGMA', 4)
        alpha = getattr(self.config, 'HEATMAP_ALPHA', 0.5)

        print(f"ç”ŸæˆTop-{viz_k}å¼‚å¸¸æ ·æœ¬çš„çƒ­åŠ›å›¾...")

        # âœ… å¤šè¿›ç¨‹å¹¶è¡Œç”Ÿæˆï¼ˆå¯é€‰ï¼Œå¦‚æœéœ€è¦è¿›ä¸€æ­¥åŠ é€Ÿï¼‰
        from concurrent.futures import ProcessPoolExecutor, as_completed

        def generate_heatmap(idx):
            """å•ä¸ªçƒ­åŠ›å›¾ç”Ÿæˆå‡½æ•°"""
            img_path = self.all_paths[idx]
            anomaly_map = anomaly_maps[idx]

            try:
                img_orig = cv2.imread(img_path)
                img_orig = cv2.cvtColor(img_orig, cv2.COLOR_BGR2RGB)
                h_orig, w_orig = img_orig.shape[:2]
            except:
                return None

            # ä¸Šé‡‡æ ·
            anomaly_map_resized = cv2.resize(anomaly_map, (w_orig, h_orig),
                                             interpolation=cv2.INTER_CUBIC)
            anomaly_map_smooth = gaussian_filter(anomaly_map_resized, sigma=sigma)

            # å½’ä¸€åŒ–
            anomaly_map_norm = (anomaly_map_smooth - anomaly_map_smooth.min()) / \
                               (anomaly_map_smooth.max() - anomaly_map_smooth.min() + 1e-8)

            # çƒ­åŠ›å›¾
            heatmap = cv2.applyColorMap(
                (anomaly_map_norm * 255).astype(np.uint8),
                cv2.COLORMAP_JET
            )
            heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)

            # å åŠ 
            overlay = (img_orig * (1 - alpha) + heatmap * alpha).astype(np.uint8)

            # ä¿å­˜
            fig, axes = plt.subplots(1, 3, figsize=(15, 5))

            axes[0].imshow(img_orig)
            axes[0].set_title('Original', fontsize=12)
            axes[0].axis('off')

            axes[1].imshow(anomaly_map_norm, cmap='jet')
            axes[1].set_title('Anomaly Map', fontsize=12)
            axes[1].axis('off')

            axes[2].imshow(overlay)
            axes[2].set_title('Overlay', fontsize=12)
            axes[2].axis('off')

            plt.suptitle(f'{Path(img_path).name}\nScore: {anomaly_map.max():.4f}',
                         fontsize=10)
            plt.tight_layout()

            save_path = viz_dir / f'{Path(img_path).stem}_heatmap.png'
            plt.savefig(save_path, dpi=150, bbox_inches='tight')
            plt.close()

            return save_path

        # âš ï¸ å¤šè¿›ç¨‹å¯èƒ½å¯¼è‡´matplotlibé—®é¢˜ï¼Œå»ºè®®å•è¿›ç¨‹æˆ–å‡å°‘vizæ•°é‡
        # å¦‚æœé‡åˆ°é—®é¢˜ï¼Œæ”¹ä¸ºæ™®é€šå¾ªç¯
        for idx in tqdm(selected_indices[:viz_k], desc="ç”Ÿæˆçƒ­åŠ›å›¾"):
            generate_heatmap(idx)

        print(f"âœ… çƒ­åŠ›å›¾å·²ä¿å­˜åˆ°: {viz_dir}")

# ==================== é…ç½®ç¤ºä¾‹ ====================
class Config:
    # è·¯å¾„
    IMAGE_DIR = r"D:\Huangda_data0801\view\2\B"      # å¾…ç­›é€‰çš„æ‰€æœ‰å›¾åƒ
    OUTPUT_DIR = "outputs_2_B"                        # è¾“å‡ºç›®å½•
    CACHE_DIR = "./cache"
    DINOV3_REPO_ROOT = r"D:\ly\dinov3-main"          # DINOv3 ä»“åº“è·¯å¾„
    DINOV3_WEIGHT_PATH = r"D:\ly\dinov3-main\dinov3_vitl16_pretrain_lvd1689m-8aa4cbdd.pth"  # æƒé‡æ–‡ä»¶

    # æ¨¡å‹
    MODEL_NAME = "dinov3_vitl16_lvd1689m_distilled"  # DINOv3æ¨¡å‹å
    IMAGE_SIZE = 512

    # Patchç‰¹å¾
    FEATURE_LAYERS = [8, 16, 23]       # æå–ViTçš„ç¬¬6/9/12å±‚ï¼ˆå¤šå°ºåº¦ï¼‰
    TOP_K_PATCHES = 10                 # å›¾åƒåˆ†æ•° = top-k patchesçš„å¹³å‡åˆ†

    # PCA
    USE_PCA = True
    PCA_DIM = 256                      # é™ç»´åç»´åº¦

    # ç­›é€‰
    USE_TOP_K = False
    TOP_K = 1000
    TOP_PERCENT = 0.1                  # ç­›é€‰top 10%

    # GPU
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    BATCH_SIZE = 32
    NUM_WORKERS = 4

    # å¯è§†åŒ–
    VIZ_TOP_K = 50                     # å¯è§†åŒ–top-50å¼‚å¸¸æ ·æœ¬çš„çƒ­åŠ›å›¾


# ==================== è¿è¡Œç¤ºä¾‹ ====================
if __name__ == "__main__":
    config = Config()

    # âœ… ä½¿ç”¨åŠ é€Ÿç‰ˆ
    detector = PatchLevelAnomalyDetectorFast(config)
    csv_file = detector.run_pipeline()

    print(f"\nğŸ‰ æ ‡æ³¨æ¸…å•: {csv_file}")
